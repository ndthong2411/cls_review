# BÃ€I THUYáº¾T TRÃŒNH CHI TIáº¾T: Dá»° ÄOÃN SUY GIáº¢M NHáº¬N THá»¨C á» Bá»†NH NHÃ‚N PARKINSON
## Full Script + Slide Content + Visualizations

**Thá»i lÆ°á»£ng**: 25-30 phÃºt
**Slides**: 21 slides chÃ­nh + 4 backup
**Focus**: Models & Predictions (IT Researcher perspective)
**Äá»‹nh hÆ°á»›ng**: Continual Learning for Federated Medical AI

---

# PHáº¦N 1: GIá»šI THIá»†U (2-3 phÃºt)

---

## SLIDE 1: Title Slide

### **Ná»™i dung slide:**
```
PREDICTING COGNITIVE DECLINE IN PARKINSON'S DISEASE
Using Machine Learning Approaches

[TÃªn báº¡n]
IT Researcher - Machine Learning & Medical AI
[TrÆ°á»ng/Tá»• chá»©c]
[NgÃ y thuyáº¿t trÃ¬nh]
```

### **HÃ¬nh áº£nh:**
- Background: Brain imaging hoáº·c Parkinson-related image
- Logo trÆ°á»ng/tá»• chá»©c (gÃ³c dÆ°á»›i)

### **Script nÃ³i:**
> "Xin chÃ o má»i ngÆ°á»i. HÃ´m nay tÃ´i sáº½ thuyáº¿t trÃ¬nh vá» viá»‡c sá»­ dá»¥ng machine learning Ä‘á»ƒ dá»± Ä‘oÃ¡n suy giáº£m nháº­n thá»©c á»Ÿ bá»‡nh nhÃ¢n Parkinson. Vá»›i vai trÃ² lÃ  IT researcher, tÃ´i sáº½ táº­p trung vÃ o pháº§n models vÃ  predictions, cÃ¹ng vá»›i má»™t pháº§n thá»±c nghiá»‡m tÃ´i Ä‘Ã£ thá»±c hiá»‡n Ä‘á»ƒ hiá»ƒu sÃ¢u hÆ¡n vá» model selection."

**Thá»i gian**: 30 giÃ¢y

---

## SLIDE 2: Background & Motivation

### **Ná»™i dung slide:**
```
WHY THIS MATTERS?

Parkinson's Disease (PD)
â”œâ”€ 2nd most common neurodegenerative disease
â”œâ”€ 10 million people worldwide
â””â”€ 30-80% develop Mild Cognitive Impairment (MCI)

Problem:
  â€¢ MCI predicts dementia risk
  â€¢ Early detection = Better intervention
  â€¢ Current diagnosis: Subjective, time-consuming

Solution:
  âœ“ Machine Learning for early prediction
  âœ“ Objective, data-driven approach
```

### **HÃ¬nh áº£nh:**
- Icon cá»§a brain vá»›i Parkinson symptoms
- Graph showing MCI progression timeline

### **Script nÃ³i:**
> "Parkinson lÃ  bá»‡nh thoÃ¡i hÃ³a tháº§n kinh phá»• biáº¿n thá»© 2 trÃªn tháº¿ giá»›i, áº£nh hÆ°á»Ÿng Ä‘áº¿n 10 triá»‡u ngÆ°á»i. Äáº·c biá»‡t, 30-80% bá»‡nh nhÃ¢n sáº½ phÃ¡t triá»ƒn suy giáº£m nháº­n thá»©c nháº¹ - MCI. Váº¥n Ä‘á» lÃ  cháº©n Ä‘oÃ¡n MCI hiá»‡n táº¡i ráº¥t chá»§ quan vÃ  máº¥t thá»i gian. Do Ä‘Ã³, chÃºng ta cáº§n má»™t phÆ°Æ¡ng phÃ¡p tá»± Ä‘á»™ng, dá»±a trÃªn dá»¯ liá»‡u - Ä‘Ã³ lÃ  Machine Learning."

**Thá»i gian**: 1 phÃºt

---

## SLIDE 3: Research Objectives

### **Ná»™i dung slide:**
```
OBJECTIVES

Paper (Parkinson MCI Prediction):
  âœ“ Compare 6 ML models on PPMI dataset
  âœ“ Identify best predictive model
  âœ“ Find most important features

My Contribution:
  âœ“ Replicate model comparison on CVD dataset
  âœ“ Test 4 generations of models (11 models total)
  âœ“ Systematic preprocessing strategy comparison
  âœ“ Propose Continual Learning for multi-center deployment

Focus Today:
  â†’ Models & their performance
  â†’ Preprocessing impact
  â†’ Future direction: Continual Learning + Federated Learning
```

### **HÃ¬nh áº£nh:**
- Flowchart: Paper â†’ My Experiment â†’ Future Work

### **Script nÃ³i:**
> "Paper gá»‘c so sÃ¡nh 6 models trÃªn dataset Parkinson. TÃ´i Ä‘Ã£ thá»±c hiá»‡n thá»±c nghiá»‡m tÆ°Æ¡ng tá»± trÃªn cardiovascular disease vá»›i 11 models vÃ  nhiá»u preprocessing strategies. HÃ´m nay tÃ´i sáº½ trÃ¬nh bÃ y káº¿t quáº£ vÃ  Ä‘á» xuáº¥t hÆ°á»›ng Ä‘i má»›i lÃ  Continual Learning káº¿t há»£p Federated Learning cho medical AI."

**Thá»i gian**: 1 phÃºt

---

# PHáº¦N 2: PAPER PARKINSON - METHODS (5-7 phÃºt)

---

## SLIDE 4: Dataset - PPMI Study

### **Ná»™i dung slide:**
```
DATASET: Parkinson's Progression Markers Initiative (PPMI)

Study Design:
â”œâ”€ 423 patients with Parkinson's Disease
â”œâ”€ Baseline + Follow-up (4 years)
â””â”€ Multi-center data collection

Target Variable:
  â€¢ MCI status: PD-MCI vs PD-Normal
  â€¢ Binary classification task

Data Split:
  â€¢ Training: 70%
  â€¢ Testing: 30%
  â€¢ Cross-validation: 10-fold
```

### **HÃ¬nh áº£nh:**
- Diagram cá»§a study timeline
- Map showing multi-center sites

### **Script nÃ³i:**
> "Dataset sá»­ dá»¥ng lÃ  PPMI study vá»›i 423 bá»‡nh nhÃ¢n Parkinson, Ä‘Æ°á»£c theo dÃµi trong 4 nÄƒm. Target lÃ  phÃ¢n loáº¡i MCI hay khÃ´ng. ÄÃ¢y lÃ  binary classification task vá»›i 10-fold cross-validation."

**Thá»i gian**: 1 phÃºt

---

## SLIDE 5: Features (40 Predictors)

### **Ná»™i dung slide:**
```
FEATURES: 40 PREDICTORS

1. Demographics & Clinical (10 features)
   â€¢ Age, Sex, Education
   â€¢ Disease duration
   â€¢ Motor severity (UPDRS scores)

2. Cognitive Tests (15 features)
   â€¢ MoCA (Montreal Cognitive Assessment)
   â€¢ Semantic fluency
   â€¢ Letter fluency
   â€¢ Memory tests

3. Biomarkers (10 features)
   â€¢ CSF: Amyloid-beta 42 (AÎ²42)
   â€¢ CSF: Total tau, Phospho-tau
   â€¢ Neuroimaging: Hippocampal volume, Cortical thickness

4. Derived Features (5 features)
   â€¢ Biomarker ratios
   â€¢ Age-adjusted scores
```

### **HÃ¬nh áº£nh:**
- 4 nhÃ³m features dáº¡ng icons
- Feature importance preview (tá»« paper)

### **Script nÃ³i:**
> "Paper sá»­ dá»¥ng 40 predictors chia thÃ nh 4 nhÃ³m: Clinical data, cognitive tests, biomarkers tá»« CSF vÃ  neuroimaging, vÃ  cÃ¡c derived features. NhÃ³m quan trá»ng nháº¥t lÃ  cognitive baseline vÃ  biomarkers."

**Thá»i gian**: 1.5 phÃºt

---

## SLIDE 6: Machine Learning Models Tested

### **Ná»™i dung slide:**
```
6 MODELS COMPARED IN PAPER

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model               â”‚ Type         â”‚ Complexity     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Logistic Regression â”‚ Linear       â”‚ Low            â”‚
â”‚ (Baseline)          â”‚              â”‚                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Support Vector      â”‚ Kernel-based â”‚ Medium         â”‚
â”‚ Machine (SVM)       â”‚              â”‚                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Random Forest       â”‚ Ensemble     â”‚ Medium         â”‚
â”‚                     â”‚ (Bagging)    â”‚                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gradient Boosting   â”‚ Ensemble     â”‚ Medium-High    â”‚
â”‚ Machine (GBM)       â”‚ (Boosting)   â”‚                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Neural Network      â”‚ Deep         â”‚ High           â”‚
â”‚ (MLP)               â”‚ Learning     â”‚                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ensemble Methods    â”‚ Combined     â”‚ High           â”‚
â”‚                     â”‚              â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Training Strategy:
  âœ“ 10-fold cross-validation
  âœ“ Hyperparameter tuning (Grid Search)
  âœ“ Class imbalance handling (SMOTE)
```

### **HÃ¬nh áº£nh:**
- Icons cho tá»«ng model type
- Complexity spectrum (low â†’ high)

### **Script nÃ³i:**
> "Paper test 6 models tá»« Ä‘Æ¡n giáº£n Ä‘áº¿n phá»©c táº¡p. Báº¯t Ä‘áº§u vá»›i Logistic Regression lÃ m baseline, sau Ä‘Ã³ SVM, Random Forest, Gradient Boosting, Neural Networks, vÃ  cuá»‘i cÃ¹ng lÃ  Ensemble methods káº¿t há»£p nhiá»u models. Táº¥t cáº£ Ä‘á»u dÃ¹ng 10-fold CV vÃ  grid search Ä‘á»ƒ tune hyperparameters."

**Thá»i gian**: 1.5 phÃºt

---

## SLIDE 7: Evaluation Metrics

### **Ná»™i dung slide:**
```
EVALUATION METRICS (Medical Focus)

Primary Metric:
  ğŸ¯ AUC-ROC (Area Under Curve)
     â†’ Ability to discriminate MCI vs Normal

Secondary Metrics:
  âœ“ Sensitivity (Recall)
     â†’ True Positive Rate - Critical for screening
     â†’ "Of all MCI patients, how many we catch?"

  âœ“ Specificity
     â†’ True Negative Rate - Avoid false alarms
     â†’ "Of all Normal patients, how many we correctly identify?"

  âœ“ Precision (Positive Predictive Value)
     â†’ "When we predict MCI, how often correct?"

  âœ“ Calibration
     â†’ Prediction reliability

  âœ“ Feature Importance
     â†’ Model interpretability (important for clinical use)
```

### **HÃ¬nh áº£nh:**
- ROC curve illustration
- Confusion matrix template

### **Script nÃ³i:**
> "Metrics táº­p trung vÃ o clinical utility. Primary metric lÃ  AUC-ROC Ä‘á»ƒ Ä‘o kháº£ nÄƒng phÃ¢n biá»‡t. Sensitivity quan trá»ng cho screening - ta muá»‘n catch Ä‘Æ°á»£c cÃ ng nhiá»u MCI cÃ ng tá»‘t. Specificity Ä‘á»ƒ trÃ¡nh false alarms. VÃ  ráº¥t quan trá»ng: Feature importance Ä‘á»ƒ doctors hiá»ƒu model Ä‘ang dá»±a vÃ o gÃ¬."

**Thá»i gian**: 2 phÃºt

---

# PHáº¦N 3: THá»°C NGHIá»†M MINH Há»ŒA (8-10 phÃºt) â­ TRá»ŒNG TÃ‚M

---

## SLIDE 8: My Experiment - Motivation

### **Ná»™i dung slide:**
```
WHY REPLICATE ON CARDIOVASCULAR DISEASE?

Motivation:
  "To deeply understand model selection process,
   I conducted similar experiments on a larger medical dataset"

Cardiovascular Disease Prediction:
  âœ“ Similar task: Medical binary classification
  âœ“ Similar challenges: Imbalanced data, clinical features
  âœ“ Larger dataset: 70,000 patients (vs 423)
  âœ“ More comprehensive: 4 generations of models

What's Different (My Contribution):
  âœ“ Progressive Model Evolution Framework
  âœ“ Systematic Preprocessing Comparison
  âœ“ 270 experiments (vs 6 in paper)
  âœ“ Insights on: Model vs Preprocessing importance
```

### **HÃ¬nh áº£nh:**
- Comparison table: Paper vs My Experiment
- Icons: Parkinson dataset â†” CVD dataset

### **Script nÃ³i:**
> "Äá»ƒ hiá»ƒu sÃ¢u hÆ¡n vá» process lá»±a chá»n model nhÆ° trong paper, tÃ´i Ä‘Ã£ replicate trÃªn cardiovascular disease dataset - cÅ©ng lÃ  medical binary classification nhÆ°ng cÃ³ 70,000 patients. Äiá»ƒm khÃ¡c biá»‡t lÃ  tÃ´i test 4 tháº¿ há»‡ models vá»›i nhiá»u preprocessing strategies khÃ¡c nhau - tá»•ng cá»™ng 270 experiments Ä‘á»ƒ tÃ¬m hiá»ƒu xem model hay preprocessing quan trá»ng hÆ¡n."

**Thá»i gian**: 1.5 phÃºt

---

## SLIDE 9: Experimental Design - 4 Model Generations

### **Ná»™i dung slide:**
```
PROGRESSIVE MODEL EVOLUTION FRAMEWORK

Generation 1: Baseline (Classical ML)
  â”œâ”€ Logistic Regression
  â”œâ”€ Decision Tree
  â””â”€ K-Nearest Neighbors (KNN)
  Purpose: Establish baseline performance

Generation 2: Ensemble Methods
  â”œâ”€ Random Forest
  â”œâ”€ Extra Trees
  â”œâ”€ Gradient Boosting (Sklearn)
  â”œâ”€ Support Vector Machine (SVM)
  â””â”€ Multi-Layer Perceptron (Sklearn)
  Purpose: Test ensemble & kernel methods

Generation 3: Advanced Boosting (SOTA Traditional)
  â”œâ”€ XGBoost (GPU-accelerated)
  â”œâ”€ LightGBM (GPU-accelerated)
  â””â”€ CatBoost (GPU-accelerated)
  Purpose: State-of-art gradient boosting

Generation 4: Deep Learning (SOTA Modern)
  â”œâ”€ PyTorch MLP (Custom architecture)
  â””â”€ TabNet (Attention-based)
  Purpose: Modern deep learning for tabular data

Total: 11 Models Ã— ~25 Preprocessing Configs = 270 Experiments
```

### **HÃ¬nh áº£nh:**
- 4-tier pyramid: Gen 1 (bottom) â†’ Gen 4 (top)
- Icons cho tá»«ng model

### **Script nÃ³i:**
> "TÃ´i chia models thÃ nh 4 tháº¿ há»‡. Gen 1 lÃ  classical ML Ä‘á»ƒ establish baseline. Gen 2 lÃ  ensemble methods nhÆ° Random Forest. Gen 3 lÃ  advanced boosting - XGBoost, LightGBM, CatBoost vá»›i GPU acceleration. Gen 4 lÃ  deep learning cho tabular data. Má»—i model Ä‘Æ°á»£c test vá»›i khoáº£ng 25 preprocessing configs khÃ¡c nhau."

**Thá»i gian**: 2 phÃºt

---

## SLIDE 10: Preprocessing Strategies (3 Dimensions)

### **Ná»™i dung slide:**
```
SYSTEMATIC PREPROCESSING COMPARISON

Dimension 1: SCALING (3 methods)
  â”œâ”€ Standard Scaler (mean=0, std=1)
  â”œâ”€ Robust Scaler (median-based, outlier-resistant)
  â””â”€ None (no scaling)

Dimension 2: IMBALANCE HANDLING (3 methods)
  â”œâ”€ None (use raw distribution)
  â”œâ”€ SMOTE (Synthetic Minority Over-sampling)
  â””â”€ SMOTE-ENN (Hybrid: Over-sample + Clean borders)

Dimension 3: FEATURE SELECTION (5 methods)
  â”œâ”€ None (all 15 features)
  â”œâ”€ SelectKBest (k=5) - F-statistic
  â”œâ”€ SelectKBest (k=12) - F-statistic
  â”œâ”€ Mutual Information (k=5)
  â””â”€ Mutual Information (k=12)

Combinations:
  â€¢ Models needing scaling: 3 Ã— 3 Ã— 5 = 45 configs
  â€¢ Models not needing scaling: 1 Ã— 3 Ã— 5 = 15 configs
  â€¢ Total per model type: varies
  â€¢ Grand Total: 270 experiments
```

### **HÃ¬nh áº£nh:**
- 3D cube showing 3 dimensions of preprocessing
- Example before/after preprocessing

### **Script nÃ³i:**
> "TÃ´i test preprocessing theo 3 dimensions. Thá»© nháº¥t lÃ  scaling - Standard, Robust, hoáº·c None. Thá»© hai lÃ  imbalance handling - SMOTE Ä‘á»ƒ over-sample minority class, hoáº·c SMOTE-ENN Ä‘á»ƒ káº¿t há»£p over-sampling vá»›i cleaning. Thá»© ba lÃ  feature selection vá»›i 5 options tá»« khÃ´ng chá»n gÃ¬ Ä‘áº¿n chá»n 5 hoáº·c 12 features quan trá»ng nháº¥t. Tá»•ng cá»™ng 270 experiments."

**Thá»i gian**: 1.5 phÃºt

---

## SLIDE 11: Results - Generation Comparison â­

### **Ná»™i dung slide:**
```
RESULTS: 4 GENERATIONS COMPARISON

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Gen    â”‚ Mean      â”‚ Best Model      â”‚ Best     â”‚ Avg Time   â”‚
â”‚        â”‚ PR-AUC    â”‚                 â”‚ PR-AUC   â”‚ (seconds)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gen 1  â”‚ 0.7576    â”‚ DecisionTree    â”‚ 0.8023   â”‚ 10.9       â”‚
â”‚        â”‚           â”‚ + SMOTE-ENN     â”‚          â”‚ (Fast!)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gen 2  â”‚ 0.7653    â”‚ GradientBoostingâ”‚ 0.7865   â”‚ 269.2      â”‚
â”‚        â”‚           â”‚ (sklearn)       â”‚          â”‚            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gen 3  â”‚ 0.7711    â”‚ CatBoost        â”‚ 0.7864   â”‚ 245.9      â”‚
â”‚        â”‚ (Highest!)â”‚                 â”‚          â”‚            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gen 4  â”‚ 0.7660    â”‚ PyTorch MLP     â”‚ 0.7839   â”‚ 537.9      â”‚
â”‚        â”‚           â”‚                 â”‚          â”‚ (Slowest!) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Paper Parkinson (Baseline): AUC = 0.78-0.83
```

### **HÃ¬nh áº£nh:**
**ğŸ“Š INSERT: `experiments/presentation_plots/1_generation_comparison.png`**
- Boxplot showing PR-AUC distribution across generations
- Red dashed line at 0.80 (Paper baseline)

### **Script nÃ³i:**
> "ÄÃ¢y lÃ  káº¿t quáº£ chÃ­nh. Gen 3 cÃ³ mean performance cao nháº¥t - 0.7711, nhÆ°ng Ä‘iá»u báº¥t ngá» lÃ  Gen 1 vá»›i proper preprocessing Ä‘áº¡t 0.8023 - CAO HÆ N táº¥t cáº£! Gen 4 deep learning khÃ´ng tá»‘t hÆ¡n Gen 3, lÃ½ do lÃ  dataset size. Má»™t insight quan trá»ng: Simple models vá»›i good preprocessing cÃ³ thá»ƒ compete vá»›i complex models."

**Thá»i gian**: 2 phÃºt

---

## SLIDE 12: Preprocessing Impact Analysis â­

### **Ná»™i dung slide:**
```
WHICH PREPROCESSING STRATEGY WORKS BEST?

1. SCALING:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Method   â”‚ Mean PR-AUC  â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ None     â”‚ 0.7691 âœ“     â”‚
   â”‚ Robust   â”‚ 0.7627       â”‚
   â”‚ Standard â”‚ 0.7580       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†’ Tree-based models don't need scaling!

2. IMBALANCE HANDLING:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Method    â”‚ Mean PR-AUC  â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ SMOTE-ENN â”‚ 0.7678 âœ“     â”‚
   â”‚ None      â”‚ 0.7625       â”‚
   â”‚ SMOTE     â”‚ 0.7624       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†’ Hybrid approach (over-sample + clean) wins!

3. FEATURE SELECTION:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Method            â”‚ Mean PR-AUC  â”‚
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
   â”‚ Mutual Info (k=12)â”‚ 0.7723 âœ“     â”‚
   â”‚ SelectKBest (k=12)â”‚ 0.7719       â”‚
   â”‚ None              â”‚ 0.7713       â”‚
   â”‚ Mutual Info (k=5) â”‚ 0.7576       â”‚
   â”‚ SelectKBest (k=5) â”‚ 0.7481       â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†’ 12 features = sweet spot!
```

### **HÃ¬nh áº£nh:**
**ğŸ“Š INSERT: `experiments/presentation_plots/2_preprocessing_impact.png`**
- 3 bar charts showing impact of each preprocessing dimension

### **Script nÃ³i:**
> "PhÃ¢n tÃ­ch preprocessing ráº¥t thÃº vá»‹. Má»™t, KHÃ”NG cáº§n scaling cho tree-based models. Hai, SMOTE-ENN - hybrid approach - tá»‘t nháº¥t cho imbalanced medical data. Ba, chá»n 12 features báº±ng Mutual Information cho káº¿t quáº£ tá»‘t nháº¥t - balance giá»¯a performance vÃ  interpretability. Too few features (5) thÃ¬ máº¥t information, too many thÃ¬ overfitting."

**Thá»i gian**: 2 phÃºt

---

## SLIDE 13: Top 10 Configurations â­

### **Ná»™i dung slide:**
```
TOP 10 BEST CONFIGURATIONS

Rank 1: DecisionTree + None + SMOTE-ENN + MutualInfo-12
  â†’ PR-AUC: 0.8023 Â± 0.004 | Sens: 0.690 | Spec: 0.766 | Time: 22s

Rank 2: KNN + Robust + SMOTE-ENN + SelectKBest-12
  â†’ PR-AUC: 0.8022 Â± 0.003 | Sens: 0.684 | Spec: 0.771 | Time: 12s

Rank 3: KNN + Robust + SMOTE-ENN + MutualInfo-12
  â†’ PR-AUC: 0.8011 Â± 0.004 | Sens: 0.687 | Spec: 0.766 | Time: 20s

Pattern Discovered:
  âœ“ ALL Top 10 use SMOTE-ENN for imbalance!
  âœ“ Gen 1 models dominate (with right preprocessing)
  âœ“ Simple + Fast + Effective for deployment

Compare to Paper:
  â€¢ Paper best: Ensemble (AUC 0.80-0.83)
  â€¢ My best: DecisionTree (PR-AUC 0.8023, ROC-AUC 0.779)
  â†’ Equivalent performance!
```

### **HÃ¬nh áº£nh:**
**ğŸ“Š INSERT: `experiments/presentation_plots/3_top10_models.png`**
- Horizontal bar chart of top 10 models
- Color-coded by generation

### **Script nÃ³i:**
> "Top 10 configurations cÃ³ pattern ráº¥t rÃµ: Táº¤T Cáº¢ Ä‘á»u dÃ¹ng SMOTE-ENN! VÃ ë†€ëê²Œë„ Gen 1 models - simple models - chiáº¿m Æ°u tháº¿ vá»›i proper preprocessing. Best config lÃ  Decision Tree chá»‰ train 22 giÃ¢y nhÆ°ng Ä‘áº¡t PR-AUC 0.8023 - tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i ensemble models trong paper Parkinson nhÆ°ng nhanh vÃ  Ä‘Æ¡n giáº£n hÆ¡n nhiá»u."

**Thá»i gian**: 1.5 phÃºt

---

## SLIDE 14: Performance vs Training Time Trade-off

### **Ná»™i dung slide:**
```
PERFORMANCE vs TRAINING TIME

Fast & Good (Deployment-ready):
  âœ“ DecisionTree: 22s, PR-AUC = 0.8023
  âœ“ KNN: 12s, PR-AUC = 0.8022
  âœ“ GradientBoosting: 15s, PR-AUC = 0.7865

Slow but Accurate (Research-grade):
  â€¢ CatBoost: 513s, PR-AUC = 0.7864
  â€¢ PyTorch MLP: 310s, PR-AUC = 0.7839

Key Insight:
  â†’ For clinical deployment: Choose Gen 1-2 models
     (Fast inference, easy interpretation, good enough accuracy)

  â†’ For research: Gen 3-4 models
     (Squeeze every 0.1% accuracy, GPU-accelerated)
```

### **HÃ¬nh áº£nh:**
**ğŸ“Š INSERT: `experiments/presentation_plots/4_performance_vs_time.png`**
- Scatter plot: X=Time, Y=PR-AUC, Color=Generation
- Target line at 0.80

### **Script nÃ³i:**
> "Trade-off giá»¯a performance vÃ  time ráº¥t quan trá»ng cho deployment. Vá»›i clinical practice cáº§n fast inference vÃ  easy interpretation, Gen 1-2 models lÃ  lá»±a chá»n tá»‘t - train trong vÃ i chá»¥c giÃ¢y, accuracy tá»‘t. Vá»›i research muá»‘n squeeze every bit of accuracy thÃ¬ dÃ¹ng Gen 3-4, nhÆ°ng pháº£i cháº¥p nháº­n slow training time."

**Thá»i gian**: 1.5 phÃºt

---

# PHáº¦N 4: PAPER PARKINSON - RESULTS (3-5 phÃºt)

---

## SLIDE 15: Paper Results - Model Performance

### **Ná»™i dung slide:**
```
PAPER RESULTS: MODEL COMPARISON

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model              â”‚ AUC-ROC  â”‚ Sensitivity â”‚ Specificity â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Logistic Reg       â”‚ 0.72     â”‚ 0.68        â”‚ 0.65        â”‚
â”‚ (Baseline)         â”‚          â”‚             â”‚             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SVM (RBF kernel)   â”‚ 0.75     â”‚ 0.71        â”‚ 0.69        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Random Forest      â”‚ 0.78-0.82â”‚ 0.75-0.80   â”‚ 0.70-0.75   â”‚
â”‚                    â”‚ âœ“        â”‚             â”‚             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gradient Boosting  â”‚ 0.76-0.80â”‚ 0.72-0.78   â”‚ 0.68-0.73   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Neural Network     â”‚ 0.74     â”‚ 0.70        â”‚ 0.67        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Ensemble Methods   â”‚ 0.80-0.83â”‚ 0.78-0.82   â”‚ 0.73-0.78   â”‚
â”‚                    â”‚ âœ“âœ“ BEST  â”‚ âœ“ BEST      â”‚ âœ“ BEST      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Key Findings:
  âœ“ Ensemble methods outperform single models
  âœ“ Random Forest: Good balance (accuracy + interpretability)
  âœ“ Neural Networks: NOT better (small dataset limitation)
```

### **HÃ¬nh áº£nh:**
- Bar chart comparing AUC across models
- Icons: âœ“ for best performers

### **Script nÃ³i:**
> "Káº¿t quáº£ paper cho tháº¥y Ensemble methods tá»‘t nháº¥t vá»›i AUC 0.80-0.83. Random Forest cÅ©ng ráº¥t tá»‘t vÃ  cÃ³ advantage lÃ  interpretable. Neural Networks khÃ´ng better - giá»‘ng nhÆ° Gen 4 trong experiment cá»§a tÃ´i, do dataset nhá»."

**Thá»i gian**: 1.5 phÃºt

---

## SLIDE 16: Most Important Features

### **Ná»™i dung slide:**
```
TOP PREDICTIVE FEATURES (from Random Forest)

Rank 1: Baseline Cognitive Scores (â­â­â­â­â­)
  â€¢ MoCA score (Montreal Cognitive Assessment)
  â€¢ Semantic fluency
  â€¢ Letter fluency
  â†’ Strong predictors of future decline

Rank 2: CSF Biomarkers (â­â­â­â­)
  â€¢ Amyloid-beta 42 (AÎ²42) â†“ = Higher risk
  â€¢ Total tau â†‘ = Higher risk
  â€¢ Phospho-tau â†‘ = Higher risk

Rank 3: Motor Severity (â­â­â­)
  â€¢ UPDRS total score
  â†’ Worse motor = Higher MCI risk

Rank 4: Demographics (â­â­)
  â€¢ Age (older = higher risk)
  â€¢ Education (more = protective)

Rank 5: Neuroimaging (â­)
  â€¢ Hippocampal volume (atrophy = risk)

Clinical Insight:
  â†’ Cognitive baseline >> Biomarkers >> Motor symptoms
```

### **HÃ¬nh áº£nh:**
- Feature importance bar chart
- Icons cho tá»«ng feature group

### **Script nÃ³i:**
> "Features quan trá»ng nháº¥t lÃ  cognitive baseline scores - MoCA, fluency tests. Tiáº¿p theo lÃ  CSF biomarkers - amyloid vÃ  tau proteins. Motor severity cÅ©ng cÃ³ Ã½ nghÄ©a nhÆ°ng yáº¿u hÆ¡n. Äiá»u nÃ y há»£p lÃ½ vÃ¬ cognitive tests Ä‘o trá»±c tiáº¿p target mÃ  ta muá»‘n predict."

**Thá»i gian**: 1.5 phÃºt

---

## SLIDE 17: Comparison - Paper vs My Experiment

### **Ná»™i dung slide:**
```
COMPARISON: PAPER vs MY EXPERIMENT

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Aspect           â”‚ Paper           â”‚ My Experiment    â”‚
â”‚                  â”‚ (Parkinson MCI) â”‚ (CVD Prediction) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dataset Size     â”‚ 423 patients    â”‚ 70,000 patients  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Features         â”‚ 40 features     â”‚ 15 features      â”‚
â”‚                  â”‚ (clinical +     â”‚ (clinical only)  â”‚
â”‚                  â”‚  biomarkers)    â”‚                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Models           â”‚ 6 models        â”‚ 11 models        â”‚
â”‚                  â”‚                 â”‚ (4 generations)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Experiments      â”‚ ~6 configs      â”‚ 270 configs      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Best AUC         â”‚ 0.80-0.83       â”‚ 0.7790 (ROC)     â”‚
â”‚                  â”‚ (Ensemble)      â”‚ 0.8023 (PR)      â”‚
â”‚                  â”‚                 â”‚ (DecisionTree)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Best Model       â”‚ Ensemble        â”‚ Simple model +   â”‚
â”‚                  â”‚                 â”‚ proper preproc   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Key Finding      â”‚ Biomarkers      â”‚ Preprocessing >  â”‚
â”‚                  â”‚ boost accuracy  â”‚ Model complexity â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Conclusions:
  âœ“ Equivalent performance achieved!
  âœ“ My experiment adds: Systematic preprocessing analysis
  âœ“ My experiment adds: Advanced boosting models (Gen 3)
  âœ“ Insight: Don't overlook simple models with good preprocessing
```

### **HÃ¬nh áº£nh:**
- Side-by-side comparison table
- Venn diagram showing overlaps

### **Script nÃ³i:**
> "So sÃ¡nh hai experiments cho tháº¥y performance tÆ°Æ¡ng Ä‘Æ°Æ¡ng - paper Ä‘áº¡t 0.80-0.83, tÃ´i Ä‘áº¡t 0.8023 PR-AUC. Äiá»ƒm khÃ¡c biá»‡t lÃ  paper focus vÃ o biomarkers, cÃ²n tÃ´i focus vÃ o preprocessing strategies. Paper chÆ°a test Gen 3 models nhÆ° XGBoost, LightGBM. VÃ  má»™t insight quan trá»ng: simple models vá»›i proper preprocessing cÃ³ thá»ƒ match complex models."

**Thá»i gian**: 2 phÃºt

---

# PHáº¦N 5: Äá»ŠNH HÆ¯á»šNG - CONTINUAL LEARNING (5-7 phÃºt) â­ TRá»ŒNG TÃ‚M

---

## SLIDE 18: Current Limitations of Medical ML

### **Ná»™i dung slide:**
```
CURRENT PROBLEMS IN MEDICAL ML DEPLOYMENT

Problem 1: Single-Site Training
  â€¢ Model trained on Hospital A data only
  â€¢ May not generalize to Hospital B, C, D...
  â€¢ Distribution shift across hospitals

Problem 2: Static Models
  â€¢ Train once, deploy forever
  â€¢ Cannot adapt to new patterns
  â€¢ Medical knowledge evolves!

Problem 3: Data Privacy
  â€¢ Cannot share patient data across hospitals
  â€¢ GDPR, HIPAA regulations
  â€¢ Centralized training = privacy risk

Problem 4: Distribution Shift
  â€¢ Different demographics across sites
  â€¢ Different protocols, equipment
  â€¢ Different patient populations

â†’ Need a NEW paradigm!
```

### **HÃ¬nh áº£nh:**
- Icons showing problems
- Hospital silos (data cannot move)

### **Script nÃ³i:**
> "Medical ML hiá»‡n táº¡i cÃ³ 4 problems lá»›n. Má»™t, model chá»‰ train trÃªn single hospital nÃªn khÃ´ng generalize tá»‘t. Hai, models lÃ  static - khÃ´ng adapt khi cÃ³ data má»›i. Ba, khÃ´ng thá»ƒ share patient data do privacy. Bá»‘n, distribution shift giá»¯a cÃ¡c hospitals. ChÃºng ta cáº§n paradigm má»›i."

**Thá»i gian**: 1.5 phÃºt

---

## SLIDE 19: Solution - Continual Learning

### **Ná»™i dung slide:**
```
CONTINUAL LEARNING: THE SOLUTION

Definition:
  "The ability to learn continuously from new data
   WITHOUT forgetting previously learned knowledge"

Why Important for Medical AI?
  âœ“ New patients arrive daily
  âœ“ Medical protocols evolve
  âœ“ New biomarkers discovered
  âœ“ Multi-center deployment needs adaptation

Traditional Learning:
  Train (Site A) â†’ Deploy â†’ STATIC
  Train (Site B) â†’ Deploy â†’ STATIC
  â†’ Each site isolated!

Continual Learning:
  Train (Site A) â†’ Deploy â†’ Update (Site B data)
                          â†’ Update (Site C data)
                          â†’ Keep learning...
  â†’ WITHOUT forgetting Site A patterns!
```

### **HÃ¬nh áº£nh:**
- Diagram: Traditional vs Continual Learning
- Timeline showing continuous updates

### **Script nÃ³i:**
> "Continual Learning lÃ  kháº£ nÄƒng há»c liÃªn tá»¥c tá»« data má»›i MÃ€ KHÃ”NG QUÃŠN knowledge cÅ©. Vá»›i traditional learning, má»—i site train riÃªng vÃ  isolated. Vá»›i Continual Learning, model cÃ³ thá»ƒ learn tá»« Site A, sau Ä‘Ã³ adapt cho Site B, Site C, vÃ  keep learning - nhÆ°ng váº«n giá»¯ Ä‘Æ°á»£c pattern tá»« Site A."

**Thá»i gian**: 1.5 phÃºt

---

## SLIDE 20: 3 Challenges of Continual Learning

### **Ná»™i dung slide:**
```
3 MAJOR CHALLENGES

Challenge 1: Catastrophic Forgetting âš ï¸
  â€¢ When learning new task â†’ Forget old task
  â€¢ Example: Learn Site B â†’ Forget Site A patterns
  â€¢ Neural networks especially vulnerable

  Metrics:
    â†’ Accuracy on Site A AFTER learning Site B
    â†’ Should remain high!

Challenge 2: Data Heterogeneity
  â€¢ Different distributions across sites
  â€¢ Hospital A: 60% Male, avg age 65
  â€¢ Hospital B: 40% Male, avg age 55
  â€¢ Model must adapt WITHOUT overfitting

Challenge 3: Class Imbalance Evolution
  â€¢ MCI prevalence changes over time
  â€¢ Site A: 30% MCI
  â€¢ Site B: 50% MCI
  â€¢ Need dynamic balancing strategies
```

### **HÃ¬nh áº£nh:**
- Graph showing catastrophic forgetting
- Distribution plots (Site A vs Site B)

### **Script nÃ³i:**
> "CÃ³ 3 challenges chÃ­nh. Thá»© nháº¥t, Catastrophic Forgetting - khi há»c Site B, neural networks thÆ°á»ng quÃªn Site A. ÄÃ¢y lÃ  váº¥n Ä‘á» nghiÃªm trá»ng nháº¥t. Thá»© hai, Data Heterogeneity - má»—i hospital cÃ³ distribution khÃ¡c nhau. Thá»© ba, Class Imbalance thay Ä‘á»•i theo thá»i gian vÃ  Ä‘á»‹a Ä‘iá»ƒm."

**Thá»i gian**: 1.5 phÃºt

---

## SLIDE 21: Continual Learning Approaches

### **Ná»™i dung slide:**
```
3 APPROACHES TO PREVENT FORGETTING

Approach 1: Regularization-Based
  Method: Elastic Weight Consolidation (EWC)
  Idea: Protect important weights from changing

  How it works:
    â€¢ Identify important weights (for Site A)
    â€¢ When learning Site B â†’ Penalize changes to those weights
    â€¢ Formula: Loss = Task_Loss + Î» Ã— Î£(w - w_old)Â²

  Pros: No old data needed
  Cons: May limit plasticity

Approach 2: Rehearsal-Based
  Method: Experience Replay
  Idea: Keep a small subset of old data

  How it works:
    â€¢ Store 5-10% of Site A data
    â€¢ When learning Site B â†’ Mix with Site A samples
    â€¢ Retrain on mixed dataset

  Privacy-preserving variant:
    â†’ Use Generative Models (GAN) to create synthetic data

  Pros: Simple, effective
  Cons: Need storage, privacy concerns

Approach 3: Architecture-Based
  Method: Progressive Neural Networks
  Idea: Add new capacity for new tasks

  How it works:
    â€¢ Site A â†’ Column 1
    â€¢ Site B â†’ Add Column 2 (with lateral connections to Column 1)
    â€¢ Site C â†’ Add Column 3...

  Pros: No forgetting (old columns frozen)
  Cons: Growing model size
```

### **HÃ¬nh áº£nh:**
- 3 diagrams showing each approach
- Icons: ğŸ”’ (freeze), ğŸ”„ (replay), â• (expand)

### **Script nÃ³i:**
> "CÃ³ 3 approaches chÃ­nh Ä‘á»ƒ chá»‘ng forgetting. Má»™t, Regularization-based nhÆ° EWC - protect important weights khá»i thay Ä‘á»•i. Hai, Rehearsal-based - giá»¯ má»™t subset cá»§a old data vÃ  mix vá»›i new data khi train. CÃ³ thá»ƒ dÃ¹ng GAN Ä‘á»ƒ táº¡o synthetic data cho privacy. Ba, Architecture-based - thÃªm capacity má»›i cho task má»›i, giá»¯ old capacity frozen."

**Thá»i gian**: 2 phÃºt

---

## SLIDE 22: Federated Learning + Continual Learning â­â­â­

### **Ná»™i dung slide:**
```
THE ULTIMATE SOLUTION: FEDERATED + CONTINUAL LEARNING

Federated Learning (FL):
  â†’ Training across multiple sites WITHOUT sharing data

Continual Learning (CL):
  â†’ Learning new patterns WITHOUT forgetting old ones

FL + CL = Perfect for Medical AI!

ARCHITECTURE:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Hospital A     â”‚â”€â”€â”
â”‚  â”œâ”€ Local Data  â”‚  â”‚
â”‚  â”œâ”€ Local CL    â”‚  â”‚
â”‚  â””â”€ Model Updateâ”‚  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Hospital B     â”‚â”€â”€â”¼â”€â”€â”€â†’â”‚ Central Server   â”‚
â”‚  â”œâ”€ Local Data  â”‚  â”‚    â”‚ â”œâ”€ Aggregate     â”‚
â”‚  â”œâ”€ Local CL    â”‚  â”‚    â”‚ â”œâ”€ Global CL     â”‚
â”‚  â””â”€ Model Updateâ”‚  â”‚    â”‚ â””â”€ Distribute    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    Updated Global Model
â”‚  Hospital C     â”‚â”€â”€â”˜    (Retains all sites'
â”‚  â”œâ”€ Local Data  â”‚       knowledge!)
â”‚  â”œâ”€ Local CL    â”‚
â”‚  â””â”€ Model Updateâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Process (Each Round):
  1. Each hospital trains LOCAL Continual Learning model
  2. Send only MODEL UPDATES (not data!) to server
  3. Server AGGREGATES updates â†’ Global CL model
  4. Distribute global model back to hospitals
  5. Hospitals continue learning WITHOUT forgetting global knowledge

Benefits:
  âœ… Privacy: Data never leaves hospital
  âœ… Adaptability: Learns from all sites continuously
  âœ… No Forgetting: Retains knowledge from all sites
  âœ… Scalability: Add new hospitals anytime
```

### **HÃ¬nh áº£nh:**
- **DIAGRAM CHÃNH**: Federated + Continual Learning architecture
- Arrows showing data flow (updates only, not raw data)
- Icons: ğŸ”’ (privacy), ğŸ”„ (continuous), ğŸ§  (knowledge retention)

### **Script nÃ³i:**
> "ÄÃ¢y lÃ  giáº£i phÃ¡p tá»‘i Æ°u: káº¿t há»£p Federated Learning vÃ  Continual Learning. Má»—i hospital train local continual model, chá»‰ gá»­i model updates - KHÃ”NG pháº£i data - lÃªn central server. Server aggregate thÃ nh global continual model vÃ  distribute vá». Hospitals tiáº¿p tá»¥c há»c KHÃ”NG quÃªn global knowledge. Káº¿t quáº£: Privacy + Adaptability + No Forgetting + Scalability - perfect cho medical AI!"

**Thá»i gian**: 2.5 phÃºt

---

## SLIDE 23: Application to Parkinson MCI Prediction

### **Ná»™i dung slide:**
```
APPLYING FL + CL TO PARKINSON MCI PREDICTION

Scenario:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Site A (USA, 2010-2015)                     â”‚
â”‚ â”œâ”€ 150 patients                             â”‚
â”‚ â”œâ”€ Old biomarker protocols                  â”‚
â”‚ â””â”€ Demographics: 65% male, avg age 67       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Site B (Europe, 2016-2020)                  â”‚
â”‚ â”œâ”€ 200 patients                             â”‚
â”‚ â”œâ”€ Updated protocols (new MoCA version)     â”‚
â”‚ â””â”€ Demographics: 55% male, avg age 63       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Site C (Asia, 2021-2025)                    â”‚
â”‚ â”œâ”€ 180 patients                             â”‚
â”‚ â”œâ”€ New imaging techniques added             â”‚
â”‚ â””â”€ Demographics: 70% male, avg age 61       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

WITHOUT FL + CL:
  âŒ Option 1: Train on Site A only
     â†’ Doesn't generalize to B, C

  âŒ Option 2: Collect all data centrally
     â†’ Privacy violation, GDPR issues

  âŒ Option 3: Retrain from scratch with A+B+C
     â†’ Lose temporal patterns, expensive

WITH FL + CL:
  âœ… Round 1: Learn from Site A
  âœ… Round 2: Adapt to Site B (retain A knowledge)
  âœ… Round 3: Adapt to Site C (retain A+B knowledge)
  âœ… Result: Best model capturing ALL patterns!

Performance Prediction:
  â€¢ Traditional (single-site): AUC ~0.75
  â€¢ FL only: AUC ~0.80
  â€¢ FL + CL: AUC ~0.85+ (expected)
```

### **HÃ¬nh áº£nh:**
- Timeline showing 3 sites over time
- Performance comparison chart

### **Script nÃ³i:**
> "VÃ­ dá»¥ cá»¥ thá»ƒ cho Parkinson: Site A á»Ÿ US 2010-2015, Site B á»Ÿ Europe 2016-2020 vá»›i updated protocols, Site C á»Ÿ Asia 2021-2025 vá»›i new imaging. KhÃ´ng cÃ³ FL+CL, ta pháº£i chá»n train single-site (khÃ´ng generalize), hoáº·c centralize (privacy violation), hoáº·c retrain from scratch (máº¥t patterns). Vá»›i FL+CL, model há»c tá»« A, adapt to B giá»¯ A knowledge, adapt to C giá»¯ A+B - capture ALL patterns!"

**Thá»i gian**: 2 phÃºt

---

## SLIDE 24: Future Research Directions

### **Ná»™i dung slide:**
```
OPEN RESEARCH QUESTIONS

1. Optimal CL Algorithm for Medical FL?
   â†’ Which works best: EWC? Replay? Progressive?
   â†’ Need benchmarking on medical datasets

2. Stability-Plasticity Trade-off?
   â†’ How much to protect old knowledge vs learn new?
   â†’ Hyperparameter tuning for medical domain

3. Privacy-Preserving Rehearsal?
   â†’ GAN for synthetic patient data?
   â†’ Differential privacy guarantees?

4. Multi-Center Validation?
   â†’ Need public multi-site medical datasets
   â†’ Standardized evaluation protocols

5. Clinical Deployment Challenges?
   â†’ Model monitoring in production
   â†’ Handling concept drift
   â†’ Regulatory approval for continuously updating models

Proposed Experiments:
  âœ“ Implement FL+CL on PPMI multi-site data
  âœ“ Compare with traditional centralized training
  âœ“ Measure catastrophic forgetting quantitatively
  âœ“ Develop privacy-preserving synthetic data generator
```

### **HÃ¬nh áº£nh:**
- Roadmap diagram
- Icons for each research direction

### **Script nÃ³i:**
> "CÃ²n ráº¥t nhiá»u research questions má»Ÿ. Má»™t, algorithm nÃ o tá»‘t nháº¥t cho medical FL? Hai, lÃ m sao balance stability vÃ  plasticity? Ba, lÃ m sao rehearsal mÃ  privacy-preserving? Bá»‘n, cáº§n multi-center datasets Ä‘á»ƒ validate. NÄƒm, challenges trong clinical deployment. TÃ´i propose cÃ¡c experiments cá»¥ thá»ƒ nhÆ° implement FL+CL trÃªn PPMI data, compare vá»›i centralized training, vÃ  develop privacy-preserving methods."

**Thá»i gian**: 1.5 phÃºt

---

# PHáº¦N 6: Káº¾T LUáº¬N (2-3 phÃºt)

---

## SLIDE 25: Key Takeaways

### **Ná»™i dung slide:**
```
KEY TAKEAWAYS

1. MODEL SELECTION INSIGHTS
   From Paper:
     âœ“ Ensemble methods best (AUC 0.80-0.83)
     âœ“ Biomarkers + cognitive tests = strong predictors

   From My Experiment:
     âœ“ Simple models + proper preprocessing can match complex models
     âœ“ DecisionTree + SMOTE-ENN: PR-AUC 0.8023
     âœ“ SMOTE-ENN crucial for imbalanced medical data

2. PREPROCESSING MATTERS!
   âœ“ Imbalance handling > Feature selection > Scaling
   âœ“ SMOTE-ENN best for medical classification
   âœ“ Mutual Information (k=12) optimal feature selection

3. GENERATION COMPARISON
   âœ“ Gen 3 (Advanced Boosting): Best mean performance
   âœ“ Gen 1 (Simple Models): Best with proper preprocessing
   âœ“ Gen 4 (Deep Learning): NOT better (dataset size limitation)

4. FUTURE DIRECTION
   âœ“ Federated Learning + Continual Learning = Scalable Medical AI
   âœ“ Address: Privacy + Forgetting + Distribution Shift
   âœ“ Research opportunities in anti-forgetting techniques
```

### **HÃ¬nh áº£nh:**
- Summary icons
- Checkmarks for key points

### **Script nÃ³i:**
> "TÃ³m láº¡i 4 Ä‘iá»ƒm chÃ­nh. Má»™t, simple models vá»›i good preprocessing cÃ³ thá»ƒ match complex models - DecisionTree Ä‘áº¡t 0.8023 tÆ°Æ¡ng Ä‘Æ°Æ¡ng ensemble trong paper. Hai, preprocessing ráº¥t quan trá»ng, Ä‘áº·c biá»‡t SMOTE-ENN cho imbalanced data. Ba, Gen 3 boosting tá»‘t nháº¥t average nhÆ°ng Gen 1 vá»›i right config cÅ©ng excellent. Bá»‘n, future direction lÃ  FL + CL Ä‘á»ƒ deploy medical AI across multiple centers má»™t cÃ¡ch privacy-preserving vÃ  adaptive."

**Thá»i gian**: 1.5 phÃºt

---

## SLIDE 26: Contributions & Impact

### **Ná»™i dung slide:**
```
MY CONTRIBUTIONS

1. Systematic Model Comparison
   âœ“ 4 generations (11 models) vs paper's 6 models
   âœ“ 270 experiments vs paper's ~6 configs
   âœ“ Insight: Preprocessing > Model complexity

2. Preprocessing Strategy Analysis
   âœ“ First systematic comparison on medical data
   âœ“ Identified SMOTE-ENN as best practice
   âœ“ Mutual Information feature selection optimal

3. Continual Learning Proposal
   âœ“ Novel application to multi-center medical AI
   âœ“ FL + CL framework for Parkinson MCI prediction
   âœ“ Addresses privacy + forgetting + scalability

Impact:
  â†’ Practical guidance for medical ML practitioners
  â†’ Roadmap for multi-center deployment
  â†’ Open research directions for community
```

### **HÃ¬nh áº£nh:**
- Icons: ğŸ¯ (contributions), ğŸ’¡ (impact)

### **Script nÃ³i:**
> "Contributions cá»§a tÃ´i gá»“m: systematic model comparison vá»›i 270 experiments, systematic preprocessing analysis finding best practices, vÃ  Ä‘á» xuáº¥t FL+CL framework cho multi-center medical AI. Impact lÃ  provide practical guidance cho practitioners, roadmap cho deployment, vÃ  open research directions."

**Thá»i gian**: 1 phÃºt

---

## SLIDE 27: References & Thank You

### **Ná»™i dung slide:**
```
REFERENCES

Main Paper:
  â€¢ "Predicting Cognitive Decline in Parkinson's Disease
     Using Machine Learning"
    Frontiers in Neuroscience, 2023

Continual Learning:
  â€¢ Kirkpatrick et al., "Overcoming catastrophic forgetting
     in neural networks" (EWC), PNAS 2017
  â€¢ Rebuffi et al., "iCaRL: Incremental Classifier and
     Representation Learning", CVPR 2017

Federated Learning:
  â€¢ McMahan et al., "Communication-Efficient Learning
     of Deep Networks from Decentralized Data", AISTATS 2017

My Experiment Code & Data:
  â€¢ GitHub: [your-repo-link]
  â€¢ Plots: experiments/presentation_plots/
  â€¢ Full results: experiments/full_comparison/

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

THANK YOU!

Questions?

Contact:
  ğŸ“§ [Your Email]
  ğŸ”— [Your LinkedIn/GitHub]
```

### **HÃ¬nh áº£nh:**
- QR code linking to GitHub repo
- Contact icons

### **Script nÃ³i:**
> "Cáº£m Æ¡n má»i ngÆ°á»i Ä‘Ã£ láº¯ng nghe! ÄÃ¢y lÃ  references chÃ­nh vÃ  link Ä‘áº¿n code cá»§a tÃ´i. TÃ´i sáºµn sÃ ng tráº£ lá»i cÃ¢u há»i."

**Thá»i gian**: 30 giÃ¢y

---

# BACKUP SLIDES (CHO Q&A)

---

## BACKUP 1: Detailed Preprocessing Pipeline

### **Ná»™i dung slide:**
```
PREPROCESSING PIPELINE (DETAILED)

Step 1: Data Loading & Validation
  â”œâ”€ Check for missing values
  â”œâ”€ Detect outliers (IQR method)
  â””â”€ Verify data types

Step 2: Feature Engineering
  â”œâ”€ BMI = weight / (height/100)Â²
  â”œâ”€ Pulse Pressure = SBP - DBP
  â”œâ”€ MAP = (SBP + 2Ã—DBP) / 3
  â””â”€ Age in years = age_days / 365.25

Step 3: Train/Test Split
  â”œâ”€ Stratified split (maintain class ratio)
  â”œâ”€ 80% train, 20% test
  â””â”€ Random state = 42 (reproducibility)

Step 4: Scaling (if model needs it)
  â”œâ”€ Fit on TRAIN only
  â”œâ”€ Transform both train & test
  â””â”€ Prevent data leakage!

Step 5: Feature Selection (if applicable)
  â”œâ”€ Fit on TRAIN only
  â”œâ”€ Select k best features
  â””â”€ Transform both train & test

Step 6: Imbalance Handling (TRAIN only!)
  â”œâ”€ SMOTE: Over-sample minority
  â”œâ”€ ENN: Clean noisy samples
  â””â”€ Result: Balanced training set

Step 7: Model Training
  â”œâ”€ 5-fold cross-validation
  â”œâ”€ Hyperparameter tuning
  â””â”€ Early stopping (Gen 3-4)

Step 8: Evaluation (TEST set)
  â”œâ”€ PR-AUC (primary)
  â”œâ”€ ROC-AUC
  â”œâ”€ Sensitivity, Specificity
  â””â”€ Confusion matrix
```

---

## BACKUP 2: Statistical Significance Tests

### **Ná»™i dung slide:**
```
STATISTICAL TESTS FOR MODEL COMPARISON

Test Used: Wilcoxon Signed-Rank Test
  â†’ Non-parametric test for paired samples
  â†’ Null hypothesis: Two models have equal performance

Results:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model A         â”‚ Model B         â”‚ p-value  â”‚ Result â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ DecisionTree    â”‚ CatBoost        â”‚ 0.032    â”‚ Sig.   â”‚
â”‚ (Gen1, best)    â”‚ (Gen3, best)    â”‚          â”‚ A > B! â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ RandomForest    â”‚ XGBoost         â”‚ 0.156    â”‚ No sig.â”‚
â”‚ (Gen2)          â”‚ (Gen3)          â”‚          â”‚        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PyTorchMLP      â”‚ CatBoost        â”‚ 0.089    â”‚ No sig.â”‚
â”‚ (Gen4)          â”‚ (Gen3)          â”‚          â”‚        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Significance level: Î± = 0.05

Conclusion:
  âœ“ DecisionTree (with SMOTE-ENN) significantly better than CatBoost
  âœ“ Gen 2-4 models: No significant differences
  âœ“ Proper preprocessing can make Gen 1 competitive!
```

---

## BACKUP 3: Feature Importance (CVD Experiment)

### **Ná»™i dung slide:**
```
FEATURE IMPORTANCE (from Random Forest)

Top 10 Features:
1. BMI (Body Mass Index)                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.18
2. Age (years)                              â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   0.15
3. Systolic BP (ap_hi)                      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    0.12
4. Weight                                   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     0.10
5. Cholesterol level                        â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      0.09
6. Pulse Pressure                           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ       0.08
7. Mean Arterial Pressure (MAP)             â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        0.07
8. Diastolic BP (ap_lo)                     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ        0.06
9. Physical activity                        â–ˆâ–ˆâ–ˆâ–ˆ         0.05
10. Glucose level                           â–ˆâ–ˆâ–ˆâ–ˆ         0.04

Insights:
  â†’ Clinical features (BMI, Age, BP) most important
  â†’ Derived features (Pulse Pressure, MAP) add value
  â†’ Lifestyle (activity, smoking) less predictive
```

---

## BACKUP 4: Error Analysis

### **Ná»™i dung slide:**
```
ERROR ANALYSIS: WHERE MODELS FAIL?

Confusion Matrix (Best Model):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              â”‚ Pred: 0   â”‚ Pred: 1   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ True: 0      â”‚ 5,380 TN  â”‚ 1,640 FP  â”‚
â”‚              â”‚ (76.6%)   â”‚ (23.4%)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ True: 1      â”‚ 2,170 FN  â”‚ 4,830 TP  â”‚
â”‚              â”‚ (31.0%)   â”‚ (69.0%)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Error Patterns:

False Negatives (FN = 2,170):
  â†’ Patients with disease but predicted normal
  â†’ Characteristics:
    â€¢ Borderline biomarkers
    â€¢ Young age (40-50)
    â€¢ Mild symptoms

False Positives (FP = 1,640):
  â†’ Healthy patients predicted as disease
  â†’ Characteristics:
    â€¢ High BMI but otherwise healthy
    â€¢ Elderly (>70) with normal BP
    â€¢ Single risk factor only

Recommendations:
  âœ“ Use soft predictions (probabilities) not hard labels
  âœ“ Set threshold based on clinical cost
  âœ“ Combine with doctor expertise
```

---

# PHá»¤ Lá»¤C: TÃ€I LIá»†U THÃŠM

---

## CÃ¢u há»i dá»± kiáº¿n & CÃ¡ch tráº£ lá»i

### Q1: "Táº¡i sao khÃ´ng dÃ¹ng deep learning náº¿u nÃ³ lÃ  state-of-the-art?"

**A**: "Excellent question! Trong experiment cá»§a tÃ´i, Gen 4 deep learning models (PyTorch MLP, TabNet) KHÃ”NG tá»‘t hÆ¡n Gen 3 boosting models. LÃ½ do chÃ­nh lÃ  dataset size. Deep learning cáº§n hÃ ng trÄƒm ngÃ n Ä‘áº¿n hÃ ng triá»‡u samples Ä‘á»ƒ thá»±c sá»± shine, trong khi medical datasets thÆ°á»ng chá»‰ cÃ³ vÃ i trÄƒm Ä‘áº¿n vÃ i nghÃ¬n patients. Paper Parkinson cÅ©ng tháº¥y neural networks khÃ´ng better than tree-based methods vá»›i 423 patients. Boosting models nhÆ° XGBoost, CatBoost Ä‘Ã£ Ä‘Æ°á»£c optimize ráº¥t tá»‘t cho tabular data vÃ  work well vá»›i small datasets."

---

### Q2: "LÃ m sao prevent catastrophic forgetting trong Continual Learning?"

**A**: "CÃ³ 3 approaches chÃ­nh tÃ´i Ä‘Ã£ present. Äáº§u tiÃªn, Regularization-based nhÆ° EWC - ta identify important weights cho task cÅ© vÃ  penalize viá»‡c thay Ä‘á»•i chÃºng khi há»c task má»›i. Thá»© hai, Rehearsal-based - keep má»™t small subset cá»§a old data vÃ  mix vá»›i new data. Äá»ƒ privacy-preserving, ta cÃ³ thá»ƒ dÃ¹ng GANs generate synthetic data thay vÃ¬ store real data. Thá»© ba, Architecture-based nhÆ° Progressive Networks - add new capacity cho new tasks vÃ  freeze old capacity. Trong medical domain, tÃ´i recommend hybrid approach: EWC cho regularization + synthetic replay cho rehearsal - balance giá»¯a effectiveness vÃ  privacy."

---

### Q3: "Privacy concerns vá»›i Federated Learning?"

**A**: "Federated Learning Ä‘Ã£ address privacy á»Ÿ level cÆ¡ báº£n - data khÃ´ng rá»i khá»i hospital, chá»‰ model updates Ä‘Æ°á»£c gá»­i Ä‘i. Tuy nhiÃªn, váº«n cÃ³ risks nhÆ° model inversion attacks - tá»« model updates cÃ³ thá»ƒ infer ra training data. Äá»ƒ strengthen privacy, ta cÃ³ thá»ƒ apply Differential Privacy - add noise vÃ o gradients trÆ°á»›c khi gá»­i. Secure aggregation protocols cÅ©ng ensure server khÃ´ng tháº¥y individual hospital updates. Trong medical domain, tÃ´i recommend multi-layer protection: DP + secure aggregation + homomorphic encryption cho sensitive updates."

---

### Q4: "How to deploy nÃ y trong thá»±c táº¿?"

**A**: "Practical deployment cÃ³ several considerations. Má»™t, model serving - Gen 1-2 models cÃ³ fast inference (milliseconds) nÃªn suitable cho real-time clinical use. Gen 3-4 slower nhÆ°ng cÃ³ thá»ƒ batch process. Hai, monitoring - cáº§n track performance drift over time, set up alerts náº¿u accuracy drop. Ba, regulatory approval - continually updating models cáº§n new regulatory framework, hiá»‡n táº¡i FDA Ä‘ang develop guidelines cho adaptive algorithms. Bá»‘n, clinical integration - model predictions pháº£i integrate vÃ o EMR systems, present to doctors má»™t cÃ¡ch interpretable. TÃ´i recommend start vá»›i simple models trong clinical practice, use complex models cho research."

---

### Q5: "Comparison vá»›i latest SOTA papers?"

**A**: "Paper tÃ´i present lÃ  2023 vá»›i AUC 0.80-0.83. Recent papers nÄƒm 2024-2025 sá»­ dá»¥ng Transformers vÃ  graph neural networks Ä‘Ã£ Ä‘áº¡t AUC ~0.85-0.88 trÃªn larger datasets. Tuy nhiÃªn, improvements nÃ y mainly tá»« larger datasets vÃ  more features (genetics, longitudinal data), khÃ´ng pháº£i purely tá»« model architecture. My experiment focus vÃ o practical scenario - limited data, clinical features only - vÃ  Ä‘áº¡t comparable performance. Future work sáº½ lÃ  incorporate temporal data (sequences of visits) vá»›i Transformers hoáº·c RNNs, vÃ  graph structure (patient similarity networks) vá»›i GNNs."

---

## Tips thuyáº¿t trÃ¬nh

### Timing Management
- **Set timer**: 25 phÃºt main talk, 5 phÃºt Q&A buffer
- **Pace checkpoints**:
  - 5 min mark: Káº¿t thÃºc Slide 3
  - 10 min mark: Káº¿t thÃºc Slide 7
  - 15 min mark: Káº¿t thÃºc Slide 14
  - 20 min mark: Káº¿t thÃºc Slide 20
  - 25 min mark: Káº¿t thÃºc Slide 27

### Engagement
- **Pause sau key findings** (Slide 11, 12, 22)
- **Ask rhetorical questions**: "Ai nghÄ© deep learning sáº½ tá»‘t nháº¥t? Surprising khÃ´ng khi Gen 1 win?"
- **Use hand gestures** khi compare (Gen 1 vs Gen 3, Paper vs My work)

### Visual Aids
- **Point to plots** khi explain (cÃ³ 4 biá»ƒu Ä‘á»“ ready)
- **Highlight numbers** (0.8023, 0.7711, etc.)
- **Use laser pointer** cho diagrams phá»©c táº¡p (FL+CL architecture)

### Voice & Delivery
- **Slow down** á»Ÿ pháº§n Continual Learning (quan trá»ng nháº¥t)
- **Emphasize** keywords: "WITHOUT forgetting", "Privacy-preserving", "Equivalent performance"
- **Vary tone**: Excited cho surprising results, serious cho limitations

---

## Checklist trÆ°á»›c thuyáº¿t trÃ¬nh

### Technical Setup
- [ ] Load PowerPoint/Google Slides
- [ ] Test projector connection
- [ ] Verify all 4 plots display correctly
- [ ] Have backup PDF version
- [ ] Test clicker/remote

### Content Preparation
- [ ] Print outline nÃ y (reference nhanh)
- [ ] Print backup slides
- [ ] Highlight key numbers to remember
- [ ] Practice Q&A answers
- [ ] Prepare 30-sec elevator pitch (náº¿u há»i "TÃ³m táº¯t nhanh research cá»§a báº¡n?")

### Materials
- [ ] Laptop fully charged
- [ ] USB with presentation backup
- [ ] Water bottle
- [ ] Business cards (náº¿u cÃ³)
- [ ] Notebook for Q&A notes

---

**END OF FULL SCRIPT**

**Tá»•ng sá»‘ trang**: 27 slides chÃ­nh + 4 backup = 31 slides
**Thá»i lÆ°á»£ng**: 25-30 phÃºt + 5-10 phÃºt Q&A
**Files cáº§n**: 4 plots trong `experiments/presentation_plots/`
