# PyTorch MLP configuration
name: mlp_torch
generation: 3  # advanced

# Architecture
architecture:
  hidden1: 128
  hidden2: 64
  hidden3: 32
  dropout1: 0.3
  dropout2: 0.2
  dropout3: 0.1

# Training
training:
  batch_size: 64
  epochs: 200
  lr: 0.001
  weight_decay: 1e-5
  optimizer: adam  # adam, adamw, sgd
  scheduler: plateau  # plateau, cosine, step, none
  early_stopping_patience: 20
  reduce_lr_patience: 10
  reduce_lr_factor: 0.5
  min_lr: 1e-7

# Class imbalance (pos_weight)
class_weight: auto  # auto or specific float

# Device
device: cuda  # cuda, cpu, auto

optuna:
  n_trials: 100
  timeout: 21600  # 6 hours
  params:
    hidden1:
      type: categorical
      choices: [64, 128, 256]
    hidden2:
      type: categorical
      choices: [32, 64, 128]
    hidden3:
      type: categorical
      choices: [16, 32, 64]
    dropout1:
      type: uniform
      low: 0.2
      high: 0.5
    dropout2:
      type: uniform
      low: 0.1
      high: 0.4
    dropout3:
      type: uniform
      low: 0.0
      high: 0.3
    lr:
      type: loguniform
      low: 1e-4
      high: 1e-2
    weight_decay:
      type: loguniform
      low: 1e-6
      high: 1e-3
    batch_size:
      type: categorical
      choices: [32, 64, 128, 256]
    optimizer:
      type: categorical
      choices: [adam, adamw, sgd]
