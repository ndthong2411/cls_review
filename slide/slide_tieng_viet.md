# ğŸ«€ Dá»° ÄOÃN Bá»†NH TIM Máº CH: PIPELINE MACHINE LEARNING TOÃ€N DIá»†N
## Tá»« Dá»¯ Liá»‡u Äáº¿n Há»— Trá»£ Quyáº¿t Äá»‹nh LÃ¢m SÃ ng

---

## SLIDE 1: SLIDE TIÃŠU Äá»€

# ğŸ«€ Dá»± ÄoÃ¡n Bá»‡nh Tim Máº¡ch
## Pipeline Machine Learning ToÃ n Diá»‡n

**Tá»« NghiÃªn Cá»©u Äáº¿n Triá»ƒn Khai LÃ¢m SÃ ng**

### TrÃ¬nh bÃ y bá»Ÿi: [TÃªn cá»§a báº¡n]
### NgÃ y: [NgÃ y hiá»‡n táº¡i]

---

## SLIDE 2: Váº¤N Äá»€

## ğŸ’” Bá»‡nh Tim Máº¡ch: ThÃ¡ch Thá»©c ToÃ n Cáº§u

### ğŸ“Š **Nhá»¯ng Con Sá»‘**
- **17.9 triá»‡u ca tá»­ vong má»—i nÄƒm** (31% tá»•ng ca tá»­ vong toÃ n cáº§u)
- **1 nghÃ¬n tá»· USD** gÃ¡nh náº·ng kinh táº¿ vÃ o nÄƒm 2030
- **80% ca tá»­ vong cÃ³ thá»ƒ phÃ²ng ngá»«a** náº¿u phÃ¡t hiá»‡n sá»›m

### ğŸ¯ **Sá»© Má»‡nh Cá»§a ChÃºng Ta**
```
PhÃ¡t Hiá»‡n Sá»›m â†’ Can Thiá»‡p Ká»‹p Thá»i â†’ Cá»©u Sá»‘ng
```

### ğŸ¤” **CÃ¢u Há»i Lá»›n**
1. LÃ m tháº¿ nÃ o dá»± Ä‘oÃ¡n rá»§i ro CVD chÃ­nh xÃ¡c?
2. PhÆ°Æ¡ng phÃ¡p ML nÃ o hoáº¡t Ä‘á»™ng tá»‘t nháº¥t?
3. LÃ m sao AI Ä‘Ã¡ng tin cáº­y cho bÃ¡c sÄ©?

---

## SLIDE 3: PHÆ¯Æ NG PHÃP Cá»¦A CHÃšNG TA

## ğŸ”¬ Pipeline ToÃ n Diá»‡n 8 BÆ°á»›c

```mermaid
graph LR
    A[Táº­p Dá»¯ Liá»‡u] --> B[Tiá»n Xá»­ LÃ½]
    B --> C[Feature Engineering]
    C --> D[Xá»­ LÃ½ Máº¥t CÃ¢n Báº±ng]
    D --> E[Training Model]
    E --> F[ÄÃ¡nh GiÃ¡]
    F --> G[Giáº£i ThÃ­ch]
    G --> H[Triá»ƒn Khai]
```

### ğŸ¯ **Äiá»u GÃ¬ Khiáº¿n Approach Cá»§a ChÃºng Ta KhÃ¡c Biá»‡t?**
- **4 Tháº¿ Há»‡ Models**: Tá»« Ä‘Æ¡n giáº£n Ä‘áº¿n state-of-the-art
- **108+ Experiments**: So sÃ¡nh toÃ n diá»‡n
- **Medical-Focused Metrics**: PR-AUC, Sensitivity, Specificity
- **Clinical Explainability**: SHAP, LIME Ä‘á»ƒ táº¡o tin tÆ°á»Ÿng bÃ¡c sÄ©
- **Real-World Ready**: TÃ­ch há»£p EHR, monitoring

---

## SLIDE 4: Dá»® LIá»†U

## ğŸ“Š Hiá»ƒu Vá» Dá»¯ Liá»‡u Cá»§a ChÃºng Ta

### ğŸ¥ **Nguá»“n Dá»¯ Liá»‡u**
| Loáº¡i | VÃ­ dá»¥ | TÃ¡c Äá»™ng |
|------|----------|--------|
| **LÃ¢m SÃ ng** | Demographics, xÃ©t nghiá»‡m, sinh hiá»‡u | Core predictors |
| **HÃ¬nh áº¢nh** | ECG, Echo, CT scans | TÄƒng 8-12% accuracy |
| **Signals** | ECG time series, HRV | Temporal patterns |
| **Wearables** | Hoáº¡t Ä‘á»™ng, giáº¥c ngá»§, stress | Monitoring liÃªn tá»¥c |

### âš ï¸ **ThÃ¡ch Thá»©c Cháº¥t LÆ°á»£ng Dá»¯ Liá»‡u**
- **Missing Data**: 5-40% across features
- **Class Imbalance**: Tá»· lá»‡ 1:10 (bá»‡nh:khá»e)
- **Multi-source Integration**: Äá»‹nh dáº¡ng khÃ¡c nhau, standards

### ğŸ’¡ **Giáº£i PhÃ¡p Cá»§a ChÃºng Ta**
- **MICE Imputation** cho missing values
- **SMOTE-ENN** cho imbalance
- **Standardized preprocessing pipeline**

---

## SLIDE 5: PIPELINE TIá»€N Xá»¬ LÃ

## ğŸ”§ Tiá»n Xá»­ LÃ½ Dá»¯ Liá»‡u: Ná»n Táº£ng

### ğŸ“‹ **Quy TrÃ¬nh Tá»«ng BÆ°á»›c**
```mermaid
graph TD
    A[Dá»¯ Liá»‡u ThÃ´] --> B[Xá»­ LÃ½ Missing]
    B --> C[Outlier Detection]
    C --> D[Scaling/Normalization]
    D --> E[Feature Encoding]
    E --> F[Dá»¯ Liá»‡u Sáº¡ch]
```

### ğŸ¯ **Ká»¹ Thuáº­t ChÃ­nh Ãp Dá»¥ng**

#### **Missing Value Strategy**
- **< 5% missing**: Median/mean imputation
- **5-20% missing**: MICE (Multiple Imputation)
- **> 20% missing**: Loáº¡i feature + indicators

#### **Scaling Methods**
- **StandardScaler**: Neural networks, SVM
- **RobustScaler**: Medical data vá»›i outliers â­
- **MinMaxScaler**: Bounded [0,1] requirements

### ğŸ“ˆ **TÃ¡c Äá»™ng**
- **86.13% â†’ 98.81%** accuracy vá»›i preprocessing Ä‘Ãºng
- **Giáº£m training time** 40%
- **Cáº£i thiá»‡n model stability**

---

## SLIDE 6: FEATURE ENGINEERING

## ğŸ¯ Feature Engineering & Selection

### ğŸ” **PhÆ°Æ¡ng PhÃ¡p Feature Selection**
```mermaid
graph LR
    A[Filter Methods] --> D[Selected Features]
    B[Wrapper Methods] --> D
    C[Embedded Methods] --> D
```

### ğŸ† **Methods Hiá»‡u Quáº£ Nháº¥t**
| Method | Accuracy | AUC | Speed |
|--------|----------|-----|-------|
| **RFE + RF** | 89.91% | 0.92 | Medium |
| **ALAN (ANOVA+Lasso)** | 88.0% | 0.898 | Fast |
| **PCA + RF** | 96.0% | 0.97 | Fast |

### ğŸ’¡ **Key Insights**
- **Medical features quan trá»ng**: Age, BP, cholesterol
- **Derived features**: BMI, pulse pressure, MAP
- **Non-linear relationships**: Captured bá»Ÿi tree-based methods
- **Feature interactions**: Quan trá»ng cho complex patterns

---

## SLIDE 7: Xá»¬ LÃ CLASS IMBALANCE

## âš–ï¸ ThÃ¡ch Thá»©c Imbalance

### ğŸš¨ **Táº¡i Sao Quan Trá»ng**
```
Model dá»± Ä‘oÃ¡n: "Má»i ngÆ°á»i Ä‘á»u khá»e"
Accuracy: 90% âœ…
NhÆ°ng: Bá» sÃ³t Táº¤T Cáº¢ bá»‡nh nhÃ¢n! âŒ
```

### ğŸ’Š **Giáº£i PhÃ¡p Cá»§a ChÃºng Ta: SMOTE-ENN**
```mermaid
graph LR
    A[Imbalanced Data] --> B[SMOTE Oversampling]
    B --> C[ENN Cleaning]
    C --> D[Balanced Dataset]
```

### ğŸ“Š **So SÃ¡nh Káº¿t Quáº£**
| Technique | Sensitivity | Specificity | F1-Score |
|-----------|-------------|-------------|----------|
| **No Handling** | 65% | 95% | 0.72 |
| **SMOTE Only** | 82% | 88% | 0.84 |
| **SMOTE-ENN** | 88% | 85% | 0.86 â­ |

### ğŸ¯ **Metric ChÃ­nh: Sensitivity â‰¥ 90%**
- **Critical cho medical screening**
- **ThÃ  false alarms cÃ²n hÆ¡n miss patients**

---

## SLIDE 8: Sá»° TIáº¾N HÃ“A MODEL

## ğŸ¤– 4 Tháº¿ Há»‡ Models

### ğŸ“ˆ **Sá»± Tiáº¿n Bá»™ Performance**
```mermaid
graph LR
    A[Gen 1: Baseline] --> B[Gen 2: Ensemble]
    B --> C[Gen 3: Advanced]
    C --> D[Gen 4: Deep Learning]
```

### ğŸ† **Tá»•ng Quan CÃ¡c Tháº¿ Há»‡**

#### **Tháº¿ Há»‡ 1: Baseline**
- **Models**: Logistic Regression, Decision Tree, KNN
- **Accuracy**: 70-85%
- **Use case**: Quick baseline, interpretable

#### **Tháº¿ Há»‡ 2: Ensemble**
- **Models**: Random Forest, Gradient Boosting, SVM
- **Accuracy**: 85-92%
- **Use case**: Balanced performance

#### **Tháº¿ Há»‡ 3: Advanced**
- **Models**: XGBoost, LightGBM, CatBoost
- **Accuracy**: 88-95%
- **Use case**: High performance

#### **Tháº¿ Há»‡ 4: Deep Learning**
- **Models**: CNN, LSTM, Hybrid CNN-LSTM
- **Accuracy**: 92-99%
- **Use case**: State-of-the-art

---

## SLIDE 9: MODELS VÃ” Äá»ŠCH

## ğŸ† Models Hiá»‡u Quáº£ Nháº¥t

### ğŸ¥‡ **Top Performers**
| Model | Tháº¿ Há»‡ | PR-AUC | Sensitivity | Thá»i Gian Training |
|-------|------------|--------|-------------|---------------|
| **XGBoost** | 3 | 0.914 | 0.912 | 45s |
| **LightGBM** | 3 | 0.909 | 0.908 | 22s |
| **CatBoost** | 3 | 0.908 | 0.907 | 65s |
| **CNN-LSTM** | 4 | 0.978 | 0.976 | 8min |

### ğŸ¯ **Cáº¥u HÃ¬nh Tá»‘i Æ¯u**
```yaml
Model: XGBoost
Preprocessing: SMOTE-ENN + Robust Scaling
Features: Top 12 (mutual information)
Hyperparameters:
  - max_depth: 10
  - learning_rate: 0.03
  - n_estimators: 2000
  - early_stopping: 100 rounds
```

### ğŸ’¡ **Key Insights**
- **XGBoost**: Balance tá»‘t nháº¥t giá»¯a performance/speed
- **Deep Learning**: Accuracy cao nháº¥t nhÆ°ng computationally expensive
- **Ensemble methods**: Consistently outperform single models

---

## SLIDE 10: Káº¾T QUáº¢ THá»°C Táº¾

## ğŸ“Š Káº¿t Quáº£ Experiments Cá»§a ChÃºng Ta

### ğŸ¯ **Dataset: Credit Card Fraud Detection (284,807 transactions)**

#### **Performance Metrics**
![Performance Comparison](experiments/presentation_plots/1_generation_comparison.png)

#### **ThÃ nh Tá»±u ChÃ­nh**
- **Best PR-AUC**: 0.854 (XGBoost + none scaler)
- **Sensitivity**: 88.5% (critical cho fraud detection)
- **Specificity**: 99.9% (minimize false positives)
- **Training Time**: <1 phÃºt cho production models

### ğŸ”„ **TÃ¡c Äá»™ng cá»§a Preprocessing**
![Preprocessing Impact](experiments/presentation_plots/2_preprocessing_impact.png)

### ğŸ† **Top 10 Models**
![Top Models](experiments/presentation_plots/3_top10_models.png)

---

## SLIDE 11: GIáº¢I THÃCH MODEL

## ğŸ” Khiáº¿n AI Hiá»ƒu ÄÆ°á»£c Cho BÃ¡c SÄ©

### ğŸ¤” **Táº¡i Sao Explainability Quan Trá»ng**
- **Clinical Trust**: BÃ¡c sÄ© cáº§n hiá»ƒu AI decisions
- **Legal Requirements**: HIPAA, GDPR compliance
- **Patient Safety**: Identify khi AI cÃ³ thá»ƒ sai

### ğŸ› ï¸ **XAI Toolkit Cá»§a ChÃºng Ta**
```mermaid
graph LR
    A[SHAP] --> D[Explanations]
    B[LIME] --> D
    C[Grad-CAM] --> D
```

### ğŸ“Š **VÃ­ Dá»¥ SHAP Analysis**
```
Rá»§i Ro Giao Dá»‹ch: CAO (85% probability)

CÃ¡c Yáº¿u Tá»‘ Rá»§i Ro ChÃ­nh:
1. Sá»‘ tiá»n giao dá»‹ch: â‚¬1,200 (+0.23 SHAP)
2. Thá»i gian giao dá»‹ch: 3:45 AM (+0.18 SHAP)
3. Khoáº£ng cÃ¡ch giao dá»‹ch: Online (+0.15 SHAP)
4. Táº§n suáº¥t giao dá»‹ch: 5 láº§n/thÃ¡ng (+0.12 SHAP)

Khuyáº¿n Nghá»‹ HÃ nh Äá»™ng:
- KhÃ³a tÃ i khoáº£n ngay láº­p tá»©c
- Kiá»ƒm tra xÃ¡c thá»±c khÃ¡ch hÃ ng
- ThÃ´ng bÃ¡o cho cÆ¡ quan giÃ¡m sÃ¡t
```

---

## SLIDE 12: TÃCH Há»¢P LÃ‚M SÃ€NG

## ğŸ¥ Tá»« Model Äáº¿n Clinical Practice

### ğŸ”„ **Deployment Pipeline**
```mermaid
graph LR
    A[Trained Model] --> B[API Service]
    B --> C[EHR Integration]
    C --> D[Clinical Dashboard]
    D --> E[Patient Care]
```

### ğŸ“± **Real-World Implementation**

#### **Edge Deployment**
- **Wearable ECG monitoring**: 97.8% accuracy
- **Real-time arrhythmia detection**: <200ms latency
- **Privacy-first**: On-device processing

#### **EHR Integration**
- **FHIR Standards**: HL7 compliant
- **Auto-risk calculation**: Background processing
- **Clinical alerts**: High-risk patient notifications

### ğŸ“ˆ **Success Metrics**
- **Early Detection**: Cáº£i thiá»‡n 35%
- **Time to Intervention**: Giáº£m 48 giá»
- **Cost Savings**: $2.3M annually

---

## SLIDE 13: MONITORING & Báº¢O TRÃŒ

## ğŸ” Giá»¯ Models ÄÃ¡ng Tin Cáº­y

### ğŸ“Š **Continuous Monitoring**
```mermaid
graph TD
    A[Model Performance] --> B[Data Drift Detection]
    B --> C[Calibration Check]
    C --> D[Bias Detection]
    D --> E[Model Retraining]
```

### ğŸš¨ **Red Flags ChÃºng TÃ´i Monitor**
- **Performance Drop**: AUC giáº£m >5%
- **Data Drift**: Distribution changes
- **Calibration Issues**: Predicted vs actual probabilities
- **Fairness Concerns**: Performance across demographics

### ğŸ”„ **Update Strategy**
- **Scheduled Retraining**: Má»—i 6 thÃ¡ng
- **Triggered Updates**: Khi performance drops
- **A/B Testing**: Validation new models
- **Version Control**: Complete audit trail

---

## SLIDE 14: BÃ€I Há»ŒC RÃšT RA

## ğŸ¯ BÃ i Há»c ÄÃ£ RÃºt Ra

### âœ… **Äiá»u Hiá»‡u Quáº£**
1. **Báº¯t Äáº§u ÄÆ¡n Giáº£n, Má»Ÿ Rá»™ng Dáº§n**: Baseline â†’ Advanced models
2. **Data Quality > Model Complexity**: 80% effort á»Ÿ preprocessing
3. **Medical Metrics Matter**: Sensitivity > Accuracy
4. **Explainability = Adoption**: Doctor trust lÃ  critical
5. **Multi-modal Data**: 8-12% performance boost

### âŒ **Äiá»u Cáº§n TrÃ¡nh**
1. **Äá»«ng Ignore Imbalance**: Medical data tá»± nhiÃªn imbalance
2. **Äá»«ng Skip External Validation**: Different hospitals matter
3. **Äá»«ng QuÃªn Ethics**: Privacy, fairness, safety first
4. **Äá»«ng Overfit**: Simple models thÆ°á»ng generalize better
5. **Äá»«ng Deploy Má»™t MÃ¬nh**: Clinician partnership essential

### ğŸš€ **HÆ°á»›ng Äi TÆ°Æ¡ng Lai**
- **Federated Learning**: Multi-hospital collaboration
- **Quantum ML**: Complex disease modeling
- **Real-time Learning**: Adaptive model updates
- **Personalized Medicine**: Patient-specific models

---

## SLIDE 15: Há»I & ÄÃP

## ğŸ¤” CÃ¢u Há»i & Tháº£o Luáº­n

### ğŸ’¬ **Äiá»ƒm Tháº£o Luáº­n**
1. **LÃ m tháº¿ nÃ o handle limited medical data?**
2. **Balance accuracy vs interpretability?**
3. **Regulatory challenges cho medical AI?**
4. **Implementation barriers trong healthcare?**

### ğŸ“§ **ThÃ´ng Tin LiÃªn Há»‡**
- **Email**: [your.email@example.com]
- **GitHub**: [github.com/yourname]
- **LinkedIn**: [linkedin.com/in/yourname]

### ğŸ™ **Cáº£m Æ¡n!**

**"CÃ¡ch tá»‘t nháº¥t Ä‘á»ƒ dá»± Ä‘oÃ¡n tÆ°Æ¡ng lai lÃ  táº¡o ra nÃ³."**
- Peter Drucker

---

## SLIDE 16: PHá»¤ Lá»¤C

## ğŸ“š Chi Tiáº¿t Ká»¹ Thuáº­t

### ğŸ”§ **Hyperparameter Tuning Results**
| Model | Best Parameters | CV Score |
|-------|----------------|----------|
| XGBoost | max_depth=10, lr=0.03, n_estimators=2000 | 0.854Â±0.012 |
| LightGBM | max_depth=10, lr=0.03, n_estimators=2000 | 0.850Â±0.015 |
| CatBoost | depth=10, lr=0.03, iterations=2000 | 0.848Â±0.018 |

### ğŸ“Š **Feature Importance (Top 10)**
1. Amount (0.23)
2. Time (0.18)
3. V1-V3 (0.15)
4. Transaction distance (0.12)
5. Category (0.10)
6. Age (0.08)
7. Gender (0.07)
8. Country (0.06)
9. Day of week (0.05)
10. Hour (0.04)

### ğŸ§ª **Experimental Setup**
- **Cross-Validation**: 5-fold Stratified
- **Random State**: 42 (reproducibility)
- **Hardware**: GPU-enabled (CUDA)
- **Software**: Python 3.8+, scikit-learn, XGBoost

---

## ğŸ“ Ghi ChÃº Cho NgÆ°á»i Thuyáº¿t TrÃ¬nh

### Tips cho Presentation:
1. **Báº¯t Ä‘áº§u vá»›i problem** - lÃ m cho relatable
2. **Show, don't just tell** - dÃ¹ng visualizations
3. **Tell a story** - data â†’ model â†’ impact
4. **Nháº¥n máº¡nh medical relevance** - khÃ´ng chá»‰ technical
5. **Practice timing** - 15-20 phÃºt tá»•ng thá»ƒ
6. **Chuáº©n bá»‹ cho questions** - especially vá» ethics vÃ  deployment

### Key Messages Ä‘á»ƒ Nháº¥n Máº¡nh:
- **Early detection saves lives**
- **AI augments, doesn't replace doctors**
- **Explainability builds trust**
- **Rigorous validation ensures safety**
- **Real-world impact lÃ  má»¥c tiÃªu**
