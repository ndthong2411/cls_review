# ü´Ä D·ª∞ ƒêO√ÅN B·ªÜNH TIM M·∫†CH: PIPELINE MACHINE LEARNING TO√ÄN DI·ªÜN
## T·ª´ D·ªØ Li·ªáu ƒê·∫øn H·ªó Tr·ª£ Quy·∫øt ƒê·ªãnh L√¢m S√†ng

---

## SLIDE 1: SLIDE TI√äU ƒê·ªÄ

# ü´Ä D·ª± ƒêo√°n B·ªánh Tim M·∫°ch
## Pipeline Machine Learning To√†n Di·ªán

**T·ª´ Nghi√™n C·ª©u ƒê·∫øn Tri·ªÉn Khai L√¢m S√†ng**

### Tr√¨nh b√†y b·ªüi: [T√™n c·ªßa b·∫°n]
### Ng√†y: [Ng√†y hi·ªán t·∫°i]

---

## SLIDE 2: V·∫§N ƒê·ªÄ

## üíî B·ªánh Tim M·∫°ch: Th√°ch Th·ª©c To√†n C·∫ßu

### üìä **Nh·ªØng Con S·ªë**
- **17.9 tri·ªáu ca t·ª≠ vong m·ªói nƒÉm** (31% t·ªïng ca t·ª≠ vong to√†n c·∫ßu)
- **1 ngh√¨n t·ª∑ USD** g√°nh n·∫∑ng kinh t·∫ø v√†o nƒÉm 2030
- **80% ca t·ª≠ vong c√≥ th·ªÉ ph√≤ng ng·ª´a** n·∫øu ph√°t hi·ªán s·ªõm

### üéØ **S·ª© M·ªánh C·ªßa Ch√∫ng Ta**
```
Ph√°t Hi·ªán S·ªõm ‚Üí Can Thi·ªáp K·ªãp Th·ªùi ‚Üí C·ª©u S·ªëng
```

### ü§î **C√¢u H·ªèi L·ªõn**
1. L√†m th·∫ø n√†o d·ª± ƒëo√°n r·ªßi ro CVD ch√≠nh x√°c?
2. Ph∆∞∆°ng ph√°p ML n√†o ho·∫°t ƒë·ªông t·ªët nh·∫•t?
3. L√†m sao AI ƒë√°ng tin c·∫≠y cho b√°c sƒ©?

---

## SLIDE 3: PH∆Ø∆†NG PH√ÅP C·ª¶A CH√öNG TA

## üî¨ Pipeline To√†n Di·ªán 8 B∆∞·ªõc

```mermaid
graph LR
    A[T·∫≠p D·ªØ Li·ªáu] --> B[Ti·ªÅn X·ª≠ L√Ω]
    B --> C[Feature Engineering]
    C --> D[X·ª≠ L√Ω M·∫•t C√¢n B·∫±ng]
    D --> E[Training Model]
    E --> F[ƒê√°nh Gi√°]
    F --> G[Gi·∫£i Th√≠ch]
    G --> H[Tri·ªÉn Khai]
```

### üéØ **ƒêi·ªÅu G√¨ Khi·∫øn Approach C·ªßa Ch√∫ng Ta Kh√°c Bi·ªát?**
- **4 Th·∫ø H·ªá Models**: T·ª´ ƒë∆°n gi·∫£n ƒë·∫øn state-of-the-art
- **108+ Experiments**: So s√°nh to√†n di·ªán
- **Medical-Focused Metrics**: PR-AUC, Sensitivity, Specificity
- **Clinical Explainability**: SHAP, LIME ƒë·ªÉ t·∫°o tin t∆∞·ªüng b√°c sƒ©
- **Real-World Ready**: T√≠ch h·ª£p EHR, monitoring

---

## SLIDE 4: D·ªÆ LI·ªÜU

## üìä Hi·ªÉu V·ªÅ D·ªØ Li·ªáu C·ªßa Ch√∫ng Ta

### üè• **Ngu·ªìn D·ªØ Li·ªáu**
| Lo·∫°i | V√≠ d·ª• | T√°c ƒê·ªông |
|------|----------|--------|
| **L√¢m S√†ng** | Demographics, x√©t nghi·ªám, sinh hi·ªáu | Core predictors |
| **H√¨nh ·∫¢nh** | ECG, Echo, CT scans | TƒÉng 8-12% accuracy |
| **Signals** | ECG time series, HRV | Temporal patterns |
| **Wearables** | Ho·∫°t ƒë·ªông, gi·∫•c ng·ªß, stress | Monitoring li√™n t·ª•c |

### ‚ö†Ô∏è **Th√°ch Th·ª©c Ch·∫•t L∆∞·ª£ng D·ªØ Li·ªáu**
- **Missing Data**: 5-40% across features
- **Class Imbalance**: T·ª∑ l·ªá 1:10 (b·ªánh:kh·ªèe)
- **Multi-source Integration**: ƒê·ªãnh d·∫°ng kh√°c nhau, standards

### üí° **Gi·∫£i Ph√°p C·ªßa Ch√∫ng Ta**
- **MICE Imputation** cho missing values
- **SMOTE-ENN** cho imbalance
- **Standardized preprocessing pipeline**

---

## SLIDE 5: PIPELINE TI·ªÄN X·ª¨ L√ù

## üîß Ti·ªÅn X·ª≠ L√Ω D·ªØ Li·ªáu: N·ªÅn T·∫£ng

### üìã **Quy Tr√¨nh T·ª´ng B∆∞·ªõc**
```mermaid
graph TD
    A[D·ªØ Li·ªáu Th√¥] --> B[X·ª≠ L√Ω Missing]
    B --> C[Outlier Detection]
    C --> D[Scaling/Normalization]
    D --> E[Feature Encoding]
    E --> F[D·ªØ Li·ªáu S·∫°ch]
```

### üéØ **K·ªπ Thu·∫≠t Ch√≠nh √Åp D·ª•ng**

#### **Missing Value Strategy**
- **< 5% missing**: Median/mean imputation
- **5-20% missing**: MICE (Multiple Imputation)
- **> 20% missing**: Lo·∫°i feature + indicators

#### **Scaling Methods**
- **StandardScaler**: Neural networks, SVM
- **RobustScaler**: Medical data v·ªõi outliers ‚≠ê
- **MinMaxScaler**: Bounded [0,1] requirements

### üìà **T√°c ƒê·ªông**
- **86.13% ‚Üí 98.81%** accuracy v·ªõi preprocessing ƒë√∫ng
- **Gi·∫£m training time** 40%
- **C·∫£i thi·ªán model stability**

---

## SLIDE 6: FEATURE ENGINEERING

## üéØ Feature Engineering & Selection

### üîç **Ph∆∞∆°ng Ph√°p Feature Selection**
```mermaid
graph LR
    A[Filter Methods] --> D[Selected Features]
    B[Wrapper Methods] --> D
    C[Embedded Methods] --> D
```

### üèÜ **Methods Hi·ªáu Qu·∫£ Nh·∫•t**
| Method | Accuracy | AUC | Speed |
|--------|----------|-----|-------|
| **RFE + RF** | 89.91% | 0.92 | Medium |
| **ALAN (ANOVA+Lasso)** | 88.0% | 0.898 | Fast |
| **PCA + RF** | 96.0% | 0.97 | Fast |

### üí° **Key Insights**
- **Medical features quan tr·ªçng**: Age, BP, cholesterol
- **Derived features**: BMI, pulse pressure, MAP
- **Non-linear relationships**: Captured b·ªüi tree-based methods
- **Feature interactions**: Quan tr·ªçng cho complex patterns

---

## SLIDE 7: X·ª¨ L√ù CLASS IMBALANCE

## ‚öñÔ∏è Th√°ch Th·ª©c Imbalance

### üö® **T·∫°i Sao Quan Tr·ªçng**
```
Model d·ª± ƒëo√°n: "M·ªçi ng∆∞·ªùi ƒë·ªÅu kh·ªèe"
Accuracy: 90% ‚úÖ
Nh∆∞ng: B·ªè s√≥t T·∫§T C·∫¢ b·ªánh nh√¢n! ‚ùå
```

### üíä **Gi·∫£i Ph√°p C·ªßa Ch√∫ng Ta: SMOTE-ENN**
```mermaid
graph LR
    A[Imbalanced Data] --> B[SMOTE Oversampling]
    B --> C[ENN Cleaning]
    C --> D[Balanced Dataset]
```

### üìä **So S√°nh K·∫øt Qu·∫£**
| Technique | Sensitivity | Specificity | F1-Score |
|-----------|-------------|-------------|----------|
| **No Handling** | 65% | 95% | 0.72 |
| **SMOTE Only** | 82% | 88% | 0.84 |
| **SMOTE-ENN** | 88% | 85% | 0.86 ‚≠ê |

### üéØ **Metric Ch√≠nh: Sensitivity ‚â• 90%**
- **Critical cho medical screening**
- **Th√† false alarms c√≤n h∆°n miss patients**

---

## SLIDE 8: S·ª∞ TI·∫æN H√ìA MODEL

## ü§ñ 4 Th·∫ø H·ªá Models

### üìà **S·ª± Ti·∫øn B·ªô Performance**
```mermaid
graph LR
    A[Gen 1: Baseline] --> B[Gen 2: Ensemble]
    B --> C[Gen 3: Advanced]
    C --> D[Gen 4: Deep Learning]
```

### üèÜ **T·ªïng Quan C√°c Th·∫ø H·ªá**

#### **Th·∫ø H·ªá 1: Baseline**
- **Models**: Logistic Regression, Decision Tree, KNN
- **Accuracy**: 70-85%
- **Use case**: Quick baseline, interpretable

#### **Th·∫ø H·ªá 2: Ensemble**
- **Models**: Random Forest, Gradient Boosting, SVM
- **Accuracy**: 85-92%
- **Use case**: Balanced performance

#### **Th·∫ø H·ªá 3: Advanced**
- **Models**: XGBoost, LightGBM, CatBoost
- **Accuracy**: 88-95%
- **Use case**: High performance

#### **Th·∫ø H·ªá 4: Deep Learning**
- **Models**: CNN, LSTM, Hybrid CNN-LSTM
- **Accuracy**: 92-99%
- **Use case**: State-of-the-art

---

## SLIDE 9: MODELS V√î ƒê·ªäCH

## üèÜ Models Hi·ªáu Qu·∫£ Nh·∫•t

### ü•á **Top Performers**
| Model | Th·∫ø H·ªá | PR-AUC | Sensitivity | Th·ªùi Gian Training |
|-------|------------|--------|-------------|---------------|
| **XGBoost** | 3 | 0.914 | 0.912 | 45s |
| **LightGBM** | 3 | 0.909 | 0.908 | 22s |
| **CatBoost** | 3 | 0.908 | 0.907 | 65s |
| **CNN-LSTM** | 4 | 0.978 | 0.976 | 8min |

### üéØ **C·∫•u H√¨nh T·ªëi ∆Øu**
```yaml
Model: XGBoost
Preprocessing: SMOTE-ENN + Robust Scaling
Features: Top 12 (mutual information)
Hyperparameters:
  - max_depth: 10
  - learning_rate: 0.03
  - n_estimators: 2000
  - early_stopping: 100 rounds
```

### üí° **Key Insights**
- **XGBoost**: Balance t·ªët nh·∫•t gi·ªØa performance/speed
- **Deep Learning**: Accuracy cao nh·∫•t nh∆∞ng computationally expensive
- **Ensemble methods**: Consistently outperform single models

---

## SLIDE 10: K·∫æT QU·∫¢ TH·ª∞C T·∫æ

## üìä K·∫øt Qu·∫£ Experiments C·ªßa Ch√∫ng Ta

### üéØ **Dataset: Credit Card Fraud Detection (284,807 transactions)**

#### **Performance Metrics**
![Performance Comparison](experiments/presentation_plots/1_generation_comparison.png)

#### **Th√†nh T·ª±u Ch√≠nh**
- **Best PR-AUC**: 0.854 (XGBoost + none scaler)
- **Sensitivity**: 88.5% (critical cho fraud detection)
- **Specificity**: 99.9% (minimize false positives)
- **Training Time**: <1 ph√∫t cho production models

### üîÑ **T√°c ƒê·ªông c·ªßa Preprocessing**
![Preprocessing Impact](experiments/presentation_plots/2_preprocessing_impact.png)

### üèÜ **Top 10 Models**
![Top Models](experiments/presentation_plots/3_top10_models.png)

---

## SLIDE 11: GI·∫¢I TH√çCH MODEL

## üîç Khi·∫øn AI Hi·ªÉu ƒê∆∞·ª£c Cho B√°c Sƒ©

### ü§î **T·∫°i Sao Explainability Quan Tr·ªçng**
- **Clinical Trust**: B√°c sƒ© c·∫ßn hi·ªÉu AI decisions
- **Legal Requirements**: HIPAA, GDPR compliance
- **Patient Safety**: Identify khi AI c√≥ th·ªÉ sai

### üõ†Ô∏è **XAI Toolkit C·ªßa Ch√∫ng Ta**
```mermaid
graph LR
    A[SHAP] --> D[Explanations]
    B[LIME] --> D
    C[Grad-CAM] --> D
```

### üìä **V√≠ D·ª• SHAP Analysis**
```
R·ªßi Ro Giao D·ªãch: CAO (85% probability)

C√°c Y·∫øu T·ªë R·ªßi Ro Ch√≠nh:
1. S·ªë ti·ªÅn giao d·ªãch: ‚Ç¨1,200 (+0.23 SHAP)
2. Th·ªùi gian giao d·ªãch: 3:45 AM (+0.18 SHAP)
3. Kho·∫£ng c√°ch giao d·ªãch: Online (+0.15 SHAP)
4. T·∫ßn su·∫•t giao d·ªãch: 5 l·∫ßn/th√°ng (+0.12 SHAP)

Khuy·∫øn Ngh·ªã H√†nh ƒê·ªông:
- Kh√≥a t√†i kho·∫£n ngay l·∫≠p t·ª©c
- Ki·ªÉm tra x√°c th·ª±c kh√°ch h√†ng
- Th√¥ng b√°o cho c∆° quan gi√°m s√°t
```

---

## SLIDE 12: T√çCH H·ª¢P L√ÇM S√ÄNG

## üè• T·ª´ Model ƒê·∫øn Clinical Practice

### üîÑ **Deployment Pipeline**
```mermaid
graph LR
    A[Trained Model] --> B[API Service]
    B --> C[EHR Integration]
    C --> D[Clinical Dashboard]
    D --> E[Patient Care]
```

### üì± **Real-World Implementation**

#### **Edge Deployment**
- **Wearable ECG monitoring**: 97.8% accuracy
- **Real-time arrhythmia detection**: <200ms latency
- **Privacy-first**: On-device processing

#### **EHR Integration**
- **FHIR Standards**: HL7 compliant
- **Auto-risk calculation**: Background processing
- **Clinical alerts**: High-risk patient notifications

### üìà **Success Metrics**
- **Early Detection**: C·∫£i thi·ªán 35%
- **Time to Intervention**: Gi·∫£m 48 gi·ªù
- **Cost Savings**: $2.3M annually

---

## SLIDE 13: MONITORING & B·∫¢O TR√å

## üîç Gi·ªØ Models ƒê√°ng Tin C·∫≠y

### üìä **Continuous Monitoring**
```mermaid
graph TD
    A[Model Performance] --> B[Data Drift Detection]
    B --> C[Calibration Check]
    C --> D[Bias Detection]
    D --> E[Model Retraining]
```

### üö® **Red Flags Ch√∫ng T√¥i Monitor**
- **Performance Drop**: AUC gi·∫£m >5%
- **Data Drift**: Distribution changes
- **Calibration Issues**: Predicted vs actual probabilities
- **Fairness Concerns**: Performance across demographics

### üîÑ **Update Strategy**
- **Scheduled Retraining**: M·ªói 6 th√°ng
- **Triggered Updates**: Khi performance drops
- **A/B Testing**: Validation new models
- **Version Control**: Complete audit trail

---

## SLIDE 14: B√ÄI H·ªåC R√öT RA

## üéØ B√†i H·ªçc ƒê√£ R√∫t Ra

### ‚úÖ **ƒêi·ªÅu Hi·ªáu Qu·∫£**
1. **B·∫Øt ƒê·∫ßu ƒê∆°n Gi·∫£n, M·ªü R·ªông D·∫ßn**: Baseline ‚Üí Advanced models
2. **Data Quality > Model Complexity**: 80% effort ·ªü preprocessing
3. **Medical Metrics Matter**: Sensitivity > Accuracy
4. **Explainability = Adoption**: Doctor trust l√† critical
5. **Multi-modal Data**: 8-12% performance boost

### ‚ùå **ƒêi·ªÅu C·∫ßn Tr√°nh**
1. **ƒê·ª´ng Ignore Imbalance**: Medical data t·ª± nhi√™n imbalance
2. **ƒê·ª´ng Skip External Validation**: Different hospitals matter
3. **ƒê·ª´ng Qu√™n Ethics**: Privacy, fairness, safety first
4. **ƒê·ª´ng Overfit**: Simple models th∆∞·ªùng generalize better
5. **ƒê·ª´ng Deploy M·ªôt M√¨nh**: Clinician partnership essential

### üöÄ **H∆∞·ªõng ƒêi T∆∞∆°ng Lai**
- **Federated Learning**: Multi-hospital collaboration
- **Quantum ML**: Complex disease modeling
- **Real-time Learning**: Adaptive model updates
- **Personalized Medicine**: Patient-specific models

---

## SLIDE 15: H·ªéI & ƒê√ÅP

## ü§î C√¢u H·ªèi & Th·∫£o Lu·∫≠n

### üí¨ **ƒêi·ªÉm Th·∫£o Lu·∫≠n**
1. **L√†m th·∫ø n√†o handle limited medical data?**
2. **Balance accuracy vs interpretability?**
3. **Regulatory challenges cho medical AI?**
4. **Implementation barriers trong healthcare?**

### üìß **Th√¥ng Tin Li√™n H·ªá**
- **Email**: [your.email@example.com]
- **GitHub**: [github.com/yourname]
- **LinkedIn**: [linkedin.com/in/yourname]

### üôè **C·∫£m ∆°n!**

**"C√°ch t·ªët nh·∫•t ƒë·ªÉ d·ª± ƒëo√°n t∆∞∆°ng lai l√† t·∫°o ra n√≥."**
- Peter Drucker

---

## SLIDE 16: PH·ª§ L·ª§C

## üìö Chi Ti·∫øt K·ªπ Thu·∫≠t

### üîß **Hyperparameter Tuning Results**
| Model | Best Parameters | CV Score |
|-------|----------------|----------|
| XGBoost | max_depth=10, lr=0.03, n_estimators=2000 | 0.854¬±0.012 |
| LightGBM | max_depth=10, lr=0.03, n_estimators=2000 | 0.850¬±0.015 |
| CatBoost | depth=10, lr=0.03, iterations=2000 | 0.848¬±0.018 |

### üìä **Feature Importance (Top 10)**
1. Amount (0.23)
2. Time (0.18)
3. V1-V3 (0.15)
4. Transaction distance (0.12)
5. Category (0.10)
6. Age (0.08)
7. Gender (0.07)
8. Country (0.06)
9. Day of week (0.05)
10. Hour (0.04)

### üß™ **Experimental Setup**
- **Cross-Validation**: 5-fold Stratified
- **Random State**: 42 (reproducibility)
- **Hardware**: GPU-enabled (CUDA)
- **Software**: Python 3.8+, scikit-learn, XGBoost

---

## üìù Ghi Ch√∫ Cho Ng∆∞·ªùi Thuy·∫øt Tr√¨nh

### Tips cho Presentation:
1. **B·∫Øt ƒë·∫ßu v·ªõi problem** - l√†m cho relatable
2. **Show, don't just tell** - d√πng visualizations
3. **Tell a story** - data ‚Üí model ‚Üí impact
4. **Nh·∫•n m·∫°nh medical relevance** - kh√¥ng ch·ªâ technical
5. **Practice timing** - 15-20 ph√∫t t·ªïng th·ªÉ
6. **Chu·∫©n b·ªã cho questions** - especially v·ªÅ ethics v√† deployment

### Key Messages ƒë·ªÉ Nh·∫•n M·∫°nh:
- **Early detection saves lives**
- **AI augments, doesn't replace doctors**
- **Explainability builds trust**
- **Rigorous validation ensures safety**
- **Real-world impact l√† m·ª•c ti√™u**
