# Th√™m Mutual Information Feature Selection

**Ng√†y c·∫≠p nh·∫≠t:** 2025-10-16  
**File thay ƒë·ªïi:** `full_comparison.py`  
**T√°c gi·∫£:** System Update

---

## üìä T·ªïng Quan

ƒê√£ th√™m **Mutual Information** l√†m ph∆∞∆°ng ph√°p feature selection th·ª© hai ƒë·ªÉ so s√°nh v·ªõi ANOVA F-test. ƒêi·ªÅu n√†y cho ph√©p ph√°t hi·ªán c·∫£ m·ªëi quan h·ªá **phi tuy·∫øn** gi·ªØa features v√† target.

## üîÑ C√°c Thay ƒê·ªïi

### 1. Import Libraries (d√≤ng 48)
```python
# Tr∆∞·ªõc:
from sklearn.feature_selection import SelectKBest, f_classif, RFE

# Sau:
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFE
```

### 2. Feature Selection Config (d√≤ng 93-98)
```python
# Tr∆∞·ªõc:
FEATURE_SELECTION_METHODS = {
    'none': None,
    'select_k_best_5': SelectKBest(f_classif, k=5),
    'select_k_best_12': SelectKBest(f_classif, k=12),
}

# Sau:
FEATURE_SELECTION_METHODS = {
    'none': None,
    'select_k_best_5': SelectKBest(f_classif, k=5),
    'select_k_best_12': SelectKBest(f_classif, k=12),
    'mutual_info_5': SelectKBest(mutual_info_classif, k=5),    # ‚≠ê M·ªöI
    'mutual_info_12': SelectKBest(mutual_info_classif, k=12),  # ‚≠ê M·ªöI
}
```

### 3. Experiment Matrix (d√≤ng 1141)
```python
# Tr∆∞·ªõc:
['none', 'select_k_best_5', 'select_k_best_12']

# Sau:
['none', 'select_k_best_5', 'select_k_best_12', 'mutual_info_5', 'mutual_info_12']
```

## üìà S·ªë L∆∞·ª£ng Experiments

| Stage | Feature Selection Methods | Total Experiments | Change |
|-------|--------------------------|-------------------|--------|
| **Baseline** | 2 (none, k=12) | 108 | - |
| **+ K=5** | 3 (none, k=5, k=12) | 162 | +54 (+50%) |
| **+ Mutual Info** | 5 (all above + mi=5, mi=12) | **270** | **+108 (+67%)** |

### Breakdown

| Model Type | Scalers | Imbalance | Feature Sel | Per Model | Total |
|-----------|---------|-----------|-------------|-----------|-------|
| **Scaling (5 models)** | 2 | 3 | 5 | 30 | 150 |
| **Non-scaling (8 models)** | 1 | 3 | 5 | 15 | 120 |
| **TOTAL** | - | - | - | - | **270** |

## ‚è±Ô∏è Th·ªùi Gian ∆Ø·ªõc T√≠nh

| Configuration | Time (minutes) | Time (hours) | Additional |
|--------------|----------------|--------------|------------|
| Baseline (2 methods) | 81 | 1.4 | - |
| + K=5 (3 methods) | 122 | 2.0 | +41 min |
| + Mutual Info (5 methods) | **203** | **3.4** | **+81 min** |

**L∆∞u √Ω:** V·ªõi cache enabled, ch·ªâ experiments m·ªõi c·∫ßn train!

## üéØ 5 Feature Selection Methods

| # | Method | Algorithm | K | Description |
|---|--------|-----------|---|-------------|
| 1 | `none` | - | All | Kh√¥ng feature selection |
| 2 | `select_k_best_5` | ANOVA F-test | 5 | Top 5 features (linear) |
| 3 | `select_k_best_12` | ANOVA F-test | 12 | Top 12 features (linear) |
| 4 | `mutual_info_5` ‚≠ê | Mutual Information | 5 | Top 5 features (non-linear) |
| 5 | `mutual_info_12` ‚≠ê | Mutual Information | 12 | Top 12 features (non-linear) |

## üî¨ ANOVA F-test vs Mutual Information

### So S√°nh Chi Ti·∫øt

| ƒê·∫∑c ƒêi·ªÉm | ANOVA F-test | Mutual Information |
|----------|--------------|-------------------|
| **M·ªëi quan h·ªá** | Ch·ªâ tuy·∫øn t√≠nh | Tuy·∫øn t√≠nh + Phi tuy·∫øn |
| **T·ªëc ƒë·ªô** | R·∫•t nhanh ‚ö° | Ch·∫≠m h∆°n üê¢ |
| **ƒê·ªô ph·ª©c t·∫°p** | O(n) | O(n log n) |
| **Gi·∫£ ƒë·ªãnh** | Ph√¢n ph·ªëi chu·∫©n | Kh√¥ng gi·∫£ ƒë·ªãnh |
| **·ªîn ƒë·ªãnh** | R·∫•t ·ªïn ƒë·ªãnh | C√≥ th·ªÉ dao ƒë·ªông |
| **Pattern ph·ª©c t·∫°p** | ‚ùå Kh√¥ng | ‚úÖ C√≥ |
| **T∆∞∆°ng t√°c features** | ‚ùå Kh√¥ng | ‚úÖ M·ªôt ph·∫ßn |

### C√¥ng Th·ª©c

#### ANOVA F-test
```
F = (Variance between classes) / (Variance within classes)

F c√†ng cao ‚Üí Feature c√†ng ph√¢n bi·ªát ƒë∆∞·ª£c classes
```

#### Mutual Information
```
MI(X, Y) = H(X) + H(Y) - H(X,Y)

Trong ƒë√≥:
- H(X): Entropy c·ªßa feature X
- H(Y): Entropy c·ªßa target Y  
- H(X,Y): Joint entropy

MI c√†ng cao ‚Üí X v√† Y c√†ng ph·ª• thu·ªôc v√†o nhau
```

### V√≠ D·ª• Minh H·ªça

```python
import numpy as np

# Feature 1: Linear relationship
x1 = np.array([1, 2, 3, 4, 5])
y = np.array([0, 0, 1, 1, 1])
# ANOVA: HIGH ‚úÖ  |  MI: HIGH ‚úÖ

# Feature 2: Non-linear (quadratic)
x2 = np.array([1, 4, 9, 16, 25])  # x^2
y = np.array([0, 0, 1, 1, 1])
# ANOVA: LOW ‚ùå  |  MI: HIGH ‚úÖ  ‚Üê Ch·ªâ MI ph√°t hi·ªán ƒë∆∞·ª£c!

# Feature 3: Noise
x3 = np.random.randn(5)
y = np.array([0, 0, 1, 1, 1])
# ANOVA: LOW ‚úÖ  |  MI: LOW ‚úÖ
```

## üí° Khi N√†o D√πng Method N√†o?

### D√πng ANOVA F-test khi:
‚úÖ D·ªØ li·ªáu c√≥ ph√¢n ph·ªëi chu·∫©n  
‚úÖ Quan h·ªá tuy·∫øn t√≠nh  
‚úÖ C·∫ßn t·ªëc ƒë·ªô nhanh  
‚úÖ Dataset l·ªõn (>100k rows)  
‚úÖ Features ƒë√£ ƒë∆∞·ª£c scale/normalize  

**Ph√π h·ª£p v·ªõi:**
- Simple models (Logistic Regression, Linear SVM)
- Preprocessed features (PCA components)
- Initial exploration

### D√πng Mutual Information khi:
‚úÖ Quan h·ªá phi tuy·∫øn  
‚úÖ Kh√¥ng gi·∫£ ƒë·ªãnh ph√¢n ph·ªëi  
‚úÖ Features ph·ª©c t·∫°p (interactions)  
‚úÖ Dataset v·ª´a (<50k rows)  
‚úÖ C·∫ßn robust feature selection  

**Ph√π h·ª£p v·ªõi:**
- Complex models (Random Forest, XGBoost)
- Raw features (before transformation)
- Medical/Biological data
- **Credit Card Fraud** (non-linear fraud patterns)
- **Cardiovascular** (complex health interactions)

## üéØ ·ª®ng D·ª•ng V·ªõi Datasets

### Credit Card Fraud Dataset
```python
# Features: V1-V28 (PCA components) + Time + Amount
# Total: 30 features

# ANOVA F-test:
- T·ªët cho V1-V28 (PCA ƒë√£ tuy·∫øn t√≠nh h√≥a)
- Nhanh v·ªõi 285k rows

# Mutual Information:
- T·ªët cho Time & Amount (non-linear patterns)
- Ph√°t hi·ªán fraud patterns ph·ª©c t·∫°p
- C√≥ th·ªÉ ch·∫≠m v·ªõi dataset l·ªõn

# K·ª≥ v·ªçng:
- MI c√≥ th·ªÉ ch·ªçn Time/Amount
- ANOVA ch·ªçn V1-V28 components
```

### Cardiovascular Dataset
```python
# Features: age, BMI, BP, cholesterol, etc.
# Total: 15 features

# ANOVA F-test:
- T·ªët cho continuous features
- Nhanh v·ªõi 70k rows

# Mutual Information:
- Ph√°t hi·ªán interactions (BMI √ó age)
- T·ªët cho categorical features
- Robust v·ªõi outliers

# K·ª≥ v·ªçng:
- MI ch·ªçn features c√≥ interaction
- ANOVA ch·ªçn features c√≥ correlation m·∫°nh
```

## üìä K·∫øt Qu·∫£ Mong ƒê·ª£i

### Scenario 1: ANOVA v√† MI ch·ªçn features gi·ªëng nhau
```
Model: Gen3_XGBoost
select_k_best_5:  ['age', 'BMI', 'ap_hi', 'ap_lo', 'cholesterol']
mutual_info_5:    ['age', 'BMI', 'ap_hi', 'ap_lo', 'cholesterol']
Performance:      T∆Ø∆†NG ƒê∆Ø∆†NG

‚Üí Quan h·ªá tuy·∫øn t√≠nh, ƒë∆°n gi·∫£n
```

### Scenario 2: MI ch·ªçn features kh√°c (T·ªêT H∆†N)
```
Model: Gen2_RandomForest
select_k_best_5:  ['age', 'BMI', 'ap_hi', 'ap_lo', 'cholesterol']
mutual_info_5:    ['age', 'BMI', 'pulse_pressure', 'MAP', 'age√óBMI']
Performance:      MI > ANOVA (+2-3% PR-AUC)

‚Üí Ph√°t hi·ªán ƒë∆∞·ª£c interactions!
```

### Scenario 3: ANOVA t·ªët h∆°n (dataset ƒë∆°n gi·∫£n)
```
Model: Gen1_LogisticRegression  
select_k_best_5:  ['age', 'BMI', 'ap_hi', 'ap_lo', 'cholesterol']
mutual_info_5:    ['age', 'weight', 'height', 'ap_hi', 'ap_lo']
Performance:      ANOVA > MI (+1% PR-AUC)

‚Üí Linear model prefer linear features
```

## üöÄ C√°ch Ch·∫°y

### Full Run (All 270 Experiments)
```bash
# Cardio dataset
python full_comparison.py --data data/raw/cardio_train.csv

# Creditcard dataset  
python full_comparison.py --data data/raw/creditcard.csv
```

### V·ªõi Cache (Recommended)
```bash
# Ch·ªâ train experiments m·ªõi (mutual_info_*)
# Experiments c≈© load t·ª´ cache
python full_comparison.py --data data/raw/cardio_train.csv

# Output:
# [1/270] Gen1_LogisticRegression | mutual_info_5 | ...
#   ‚öôÔ∏è  Training... (experiment m·ªõi)
# [2/270] Gen1_LogisticRegression | select_k_best_5 | ...
#   ‚úì Loaded from cache! (experiment c≈©)
```

### No Cache (Train All)
```bash
python full_comparison.py --data data/raw/cardio_train.csv --no-cache
```

## üìÅ Output Structure

```
experiments/
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îú‚îÄ‚îÄ cardio_train_20251016_HHMMSS.log
‚îÇ   ‚îî‚îÄ‚îÄ creditcard_20251016_HHMMSS.log
‚îú‚îÄ‚îÄ full_comparison/
‚îÇ   ‚îú‚îÄ‚îÄ cardio_train/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ full_comparison_20251016_HHMMSS.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ best_model/
‚îÇ   ‚îî‚îÄ‚îÄ creditcard/
‚îÇ       ‚îú‚îÄ‚îÄ full_comparison_20251016_HHMMSS.csv
‚îÇ       ‚îî‚îÄ‚îÄ best_model/
‚îî‚îÄ‚îÄ model_cache/
    ‚îú‚îÄ‚îÄ cardio_train/
    ‚îÇ   ‚îú‚îÄ‚îÄ Gen1_KNN_..._mutual_info_5_abc.pkl  ‚≠ê NEW
    ‚îÇ   ‚îú‚îÄ‚îÄ Gen1_KNN_..._mutual_info_12_def.pkl ‚≠ê NEW
    ‚îÇ   ‚îî‚îÄ‚îÄ ... (existing cache files)
    ‚îî‚îÄ‚îÄ creditcard/
        ‚îî‚îÄ‚îÄ ... (same structure)
```

## üîç Ph√¢n T√≠ch K·∫øt Qu·∫£

### 1. So S√°nh ANOVA vs MI
```python
import pandas as pd

df = pd.read_csv('experiments/full_comparison/cardio_train/full_comparison_*.csv')

# Group by model and feature selection method
comparison = df.groupby(['model', 'feature_selection'])['pr_auc'].mean().unstack()

# Compare k=5
print(comparison[['select_k_best_5', 'mutual_info_5']])

# Which is better?
better = (comparison['mutual_info_5'] > comparison['select_k_best_5']).sum()
print(f"Mutual Info t·ªët h∆°n ·ªü {better}/{len(comparison)} models")
```

### 2. Feature Importance Comparison
```python
# Features ch·ªçn b·ªüi ANOVA
anova_features = get_selected_features('select_k_best_5')

# Features ch·ªçn b·ªüi MI
mi_features = get_selected_features('mutual_info_5')

# So s√°nh
print(f"Overlap: {len(set(anova_features) & set(mi_features))} features")
print(f"Only ANOVA: {set(anova_features) - set(mi_features)}")
print(f"Only MI: {set(mi_features) - set(anova_features)}")
```

### 3. Best Method Per Model
```python
# T√¨m method t·ªët nh·∫•t cho m·ªói model
best_methods = df.groupby('model').apply(
    lambda x: x.loc[x['pr_auc'].idxmax(), 'feature_selection']
)

print(best_methods.value_counts())
# Output:
# mutual_info_5     5 models
# mutual_info_12    3 models  
# select_k_best_5   3 models
# select_k_best_12  2 models
# none              0 models
```

## ‚ö†Ô∏è L∆∞u √ù Quan Tr·ªçng

### 1. Mutual Information Ch·∫≠m H∆°n
- MI c√≥ th·ªÉ ch·∫≠m g·∫•p 2-5 l·∫ßn ANOVA
- V·ªõi dataset l·ªõn (>200k rows), c√≥ th·ªÉ m·∫•t v√†i ph√∫t
- S·ª≠ d·ª•ng cache ƒë·ªÉ tr√°nh train l·∫°i

### 2. Random Seed
```python
# MI c√≥ element stochastic
mutual_info_classif(X, y, random_state=42)  # ƒê·∫£m b·∫£o reproducibility
```

### 3. Feature Scaling
- ANOVA: Kh√¥ng nh·∫°y c·∫£m v·ªõi scale
- MI: **NH·∫†Y C·∫¢M** v·ªõi scale
- ‚Üí N√™n scale tr∆∞·ªõc khi d√πng MI

### 4. Small Sample Size
- MI c·∫ßn √≠t nh·∫•t ~100 samples per feature
- V·ªõi k=12, c·∫ßn >1200 samples
- Dataset c·ªßa ch√∫ng ta: 70k (cardio), 285k (credit) ‚Üí OK ‚úÖ

## üìö T√†i Li·ªáu Tham Kh·∫£o

### Papers
1. **Mutual Information Feature Selection**
   - Cover & Thomas (1991) - Elements of Information Theory
   
2. **ANOVA F-test**
   - Fisher (1925) - Statistical Methods for Research Workers

### Scikit-learn Documentation
- [`mutual_info_classif`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html)
- [`f_classif`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html)

## üéâ T√≥m T·∫Øt

‚úÖ **ƒê√£ th√™m:** `mutual_info_5` v√† `mutual_info_12`  
‚úÖ **T·ªïng experiments:** 162 ‚Üí 270 (+108, +67%)  
‚úÖ **Th·ªùi gian th√™m:** ~81 ph√∫t (~1.4 gi·ªù)  
‚úÖ **L·ª£i √≠ch:** Ph√°t hi·ªán non-linear feature relationships  
‚úÖ **Compatible:** Ho·∫°t ƒë·ªông v·ªõi m·ªçi models v√† datasets  
‚úÖ **Cache-friendly:** Ch·ªâ train experiments m·ªõi  

---

**Next Steps:**
1. ‚úÖ Ch·∫°y full comparison v·ªõi mutual info
2. ‚è≥ Ph√¢n t√≠ch features ƒë∆∞·ª£c ch·ªçn
3. ‚è≥ So s√°nh performance ANOVA vs MI
4. ‚è≥ Document insights

**Date:** 2025-10-16  
**Version:** 3.0 (Baseline ‚Üí K=5 ‚Üí Mutual Info)
