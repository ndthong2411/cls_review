# cls_review - Cardiovascular Disease Prediction

A research-friendly pipeline to compare preprocessing methods, imbalance strategies, feature selection, and a progressive model zoo (ML + DL) on the Kaggle Cardiovascular Disease dataset.

This project uses a **PyTorch-first approach** for deep learning models and provides a **Streamlit demo** for interactive model comparison.

## ğŸš€ Quick Start

### 1. Setup Environment

```powershell
# Windows PowerShell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

If you have a CUDA-capable GPU and want a CUDA PyTorch build, visit https://pytorch.org/get-started/locally/ and reinstall torch/torchvision accordingly.

### 2. Download Dataset

Download the Cardiovascular Disease dataset from Kaggle and place it in the data folder:

1. Go to: https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset
2. Download `cardio_train.csv`
3. Place it in: `data/raw/cardio_train.csv`

### 3. Quick Training (Recommended for First Run)

Train baseline models quickly to generate results for the demo:

```powershell
python quickstart.py
```

This will:
- Load and preprocess the dataset
- Train 4-6 models (depending on installed libraries)
- Perform 5-fold cross-validation
- Save results to `experiments/results_summary.csv`
- Take approximately 5-15 minutes

### 4. Launch Streamlit Demo

```powershell
streamlit run app.py
```

The demo includes:
- **Data Explorer**: Visualize dataset statistics and distributions
- **Model Training**: Configure preprocessing and model parameters
- **Results Comparison**: Compare performance across model generations
- **Make Predictions**: Use trained models for predictions (coming soon)

## ğŸ“Š Project Structure

```
cls_review/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Place cardio_train.csv here
â”‚   â”œâ”€â”€ interim/          # Intermediate processed data
â”‚   â””â”€â”€ processed/        # Final processed datasets
â”œâ”€â”€ notebooks/            # Jupyter notebooks for EDA
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ configs/          # Hydra configuration files
â”‚   â”œâ”€â”€ data/             # Data loading and validation
â”‚   â”œâ”€â”€ preprocessing/    # Preprocessing transformers
â”‚   â”œâ”€â”€ features/         # Feature engineering and selection
â”‚   â”œâ”€â”€ imbalance/        # Imbalance handling methods
â”‚   â”œâ”€â”€ models/           # Model definitions and wrappers
â”‚   â”œâ”€â”€ training/         # Training loops and CV
â”‚   â”œâ”€â”€ evaluation/       # Metrics and statistical tests
â”‚   â”œâ”€â”€ utils/            # Utilities (logging, seeding)
â”‚   â””â”€â”€ experiment/       # Experiment orchestration
â”œâ”€â”€ experiments/          # Results, figures, reports
â”œâ”€â”€ mlruns/               # MLflow tracking data
â”œâ”€â”€ app.py                # Streamlit demo application
â”œâ”€â”€ quickstart.py         # Quick training script
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ claude.md             # Full project plan
â””â”€â”€ README.md             # This file
```

## ğŸ¯ Progressive Model Comparison

The pipeline evaluates models across **3 generations**:

### Generation 1: Baseline (70-85% accuracy)
- Logistic Regression
- Decision Tree
- K-Nearest Neighbors

### Generation 2: Intermediate (85-92% accuracy)
- Random Forest
- SVM (RBF kernel)
- Gradient Boosting

### Generation 3: Advanced (88-96% accuracy)
- XGBoost
- LightGBM
- CatBoost
- MLP (PyTorch)

## ğŸ”¬ Full Experiment Pipeline

For running complete experiments with Optuna hyperparameter tuning:

### Phase 1: Baseline Exploration
```powershell
python -m src.experiment.run_phase --phase=baseline
```

### Phase 2: Intermediate Optimization
```powershell
python -m src.experiment.run_phase --phase=intermediate
```

### Phase 3: Advanced Tuning
```powershell
python -m src.experiment.run_phase --phase=advanced
```

### View MLflow Results
```powershell
mlflow ui --backend-store-uri .\mlruns
```

## ğŸ“ˆ Key Features

- **Progressive Comparison**: Compare simple baselines to SOTA models
- **Preprocessing Variants**: Test multiple missing value, outlier, and scaling strategies
- **Imbalance Handling**: SMOTE, ADASYN, SMOTE-ENN, class weights
- **Feature Selection**: RFE, L1 regularization, tree-based importance
- **Cross-Validation**: Stratified K-fold with proper pipeline integration
- **Hyperparameter Tuning**: Optuna with TPE sampler and Hyperband pruning
- **Tracking**: MLflow for experiment management
- **Statistical Testing**: McNemar and DeLong tests for model comparison
- **Interactive Demo**: Streamlit app for visualization and exploration

## ğŸ“ Configuration

All experiments are configured via Hydra YAML files in `src/configs/`:

- `config.yaml`: Main configuration
- `preprocessing/`: Missing values, outliers, scaling, encoding
- `features/`: Feature selection and PCA
- `imbalance/`: SMOTE, ADASYN, class weights
- `model/`: Model-specific hyperparameters and search spaces

## ğŸ¨ Visualizations

The pipeline generates:
- ROC and Precision-Recall curves
- Confusion matrices
- Feature importance plots
- SHAP summary plots
- Generation comparison boxplots
- Training time vs performance scatter plots
- Correlation heatmaps

## ğŸ¤ Contributing

Contributions are welcome! Areas for improvement:
- Additional model architectures (TabNet, Neural ODEs)
- More feature engineering strategies
- Federated learning implementation
- Advanced interpretation techniques

## ğŸ“š References

See `claude.md` for detailed methodology, literature review, and implementation notes.

## ğŸ“„ License

MIT License - see LICENSE file for details.

---

**Built with**: Python â€¢ PyTorch â€¢ scikit-learn â€¢ XGBoost â€¢ Streamlit â€¢ MLflow â€¢ Optuna
