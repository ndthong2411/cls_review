# cls_review - Cardiovascular Disease Prediction

A research-friendly pipeline to compare preprocessing methods, imbalance strategies, feature selection, and a progressive model zoo (ML + DL) on the Kaggle Cardiovascular Disease dataset.

This project uses a **PyTorch-first approach** for deep learning models and provides a **Streamlit demo** for interactive model comparison.

## 🚀 Quick Start

### 1. Setup Environment

```powershell
# Windows PowerShell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

If you have a CUDA-capable GPU and want a CUDA PyTorch build, visit https://pytorch.org/get-started/locally/ and reinstall torch/torchvision accordingly.

### 2. Download Dataset

Download the Cardiovascular Disease dataset from Kaggle and place it in the data folder:

1. Go to: https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset
2. Download `cardio_train.csv`
3. Place it in: `data/raw/cardio_train.csv`

### 3. Quick Training (Recommended for First Run)

Train baseline models quickly to generate results for the demo:

```powershell
python quickstart.py
```

This will:
- Load and preprocess the dataset
- Train 4-6 models (depending on installed libraries)
- Perform 5-fold cross-validation
- Save results to `experiments/results_summary.csv`
- Take approximately 5-15 minutes

### 4. Launch Streamlit Demo

```powershell
streamlit run app.py
```

The demo includes:
- **Data Explorer**: Visualize dataset statistics and distributions
- **Model Training**: Configure preprocessing and model parameters
- **Results Comparison**: Compare performance across model generations
- **Make Predictions**: Use trained models for predictions (coming soon)

## 📊 Project Structure

```
cls_review/
├── data/
│   ├── raw/              # Place cardio_train.csv here
│   ├── interim/          # Intermediate processed data
│   └── processed/        # Final processed datasets
├── notebooks/            # Jupyter notebooks for EDA
├── src/
│   ├── configs/          # Hydra configuration files
│   ├── data/             # Data loading and validation
│   ├── preprocessing/    # Preprocessing transformers
│   ├── features/         # Feature engineering and selection
│   ├── imbalance/        # Imbalance handling methods
│   ├── models/           # Model definitions and wrappers
│   ├── training/         # Training loops and CV
│   ├── evaluation/       # Metrics and statistical tests
│   ├── utils/            # Utilities (logging, seeding)
│   └── experiment/       # Experiment orchestration
├── experiments/          # Results, figures, reports
├── mlruns/               # MLflow tracking data
├── app.py                # Streamlit demo application
├── quickstart.py         # Quick training script
├── requirements.txt      # Python dependencies
├── claude.md             # Full project plan
└── README.md             # This file
```

## 🎯 Progressive Model Comparison

The pipeline evaluates models across **3 generations**:

### Generation 1: Baseline (70-85% accuracy)
- Logistic Regression
- Decision Tree
- K-Nearest Neighbors

### Generation 2: Intermediate (85-92% accuracy)
- Random Forest
- SVM (RBF kernel)
- Gradient Boosting

### Generation 3: Advanced (88-96% accuracy)
- XGBoost
- LightGBM
- CatBoost
- MLP (PyTorch)

## 🔬 Full Experiment Pipeline

For running complete experiments with Optuna hyperparameter tuning:

### Phase 1: Baseline Exploration
```powershell
python -m src.experiment.run_phase --phase=baseline
```

### Phase 2: Intermediate Optimization
```powershell
python -m src.experiment.run_phase --phase=intermediate
```

### Phase 3: Advanced Tuning
```powershell
python -m src.experiment.run_phase --phase=advanced
```

### View MLflow Results
```powershell
mlflow ui --backend-store-uri .\mlruns
```

## 📈 Key Features

- **Progressive Comparison**: Compare simple baselines to SOTA models
- **Preprocessing Variants**: Test multiple missing value, outlier, and scaling strategies
- **Imbalance Handling**: SMOTE, ADASYN, SMOTE-ENN, class weights
- **Feature Selection**: RFE, L1 regularization, tree-based importance
- **Cross-Validation**: Stratified K-fold with proper pipeline integration
- **Hyperparameter Tuning**: Optuna with TPE sampler and Hyperband pruning
- **Tracking**: MLflow for experiment management
- **Statistical Testing**: McNemar and DeLong tests for model comparison
- **Interactive Demo**: Streamlit app for visualization and exploration

## 📝 Configuration

All experiments are configured via Hydra YAML files in `src/configs/`:

- `config.yaml`: Main configuration
- `preprocessing/`: Missing values, outliers, scaling, encoding
- `features/`: Feature selection and PCA
- `imbalance/`: SMOTE, ADASYN, class weights
- `model/`: Model-specific hyperparameters and search spaces

## 🎨 Visualizations

The pipeline generates:
- ROC and Precision-Recall curves
- Confusion matrices
- Feature importance plots
- SHAP summary plots
- Generation comparison boxplots
- Training time vs performance scatter plots
- Correlation heatmaps

## 🤝 Contributing

Contributions are welcome! Areas for improvement:
- Additional model architectures (TabNet, Neural ODEs)
- More feature engineering strategies
- Federated learning implementation
- Advanced interpretation techniques

## 📚 References

See `claude.md` for detailed methodology, literature review, and implementation notes.

## 📄 License

MIT License - see LICENSE file for details.

---

**Built with**: Python • PyTorch • scikit-learn • XGBoost • Streamlit • MLflow • Optuna
