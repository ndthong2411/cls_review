# BÃ€I THUYáº¾T TRÃŒNH: Dá»° ÄOÃN SUY GIáº¢M NHáº¬N THá»¨C á» Bá»†NH NHÃ‚N PARKINSON
## TÃ­ch há»£p: Paper Analysis + Thá»±c nghiá»‡m Cardiovascular Disease

---

## **PHáº¦N 1: GIá»šI THIá»†U** (2-3 phÃºt)

### Slide 1: Title
- TiÃªu Ä‘á»: "Predicting Cognitive Decline in Parkinson's Disease Using Machine Learning"
- TÃªn ngÆ°á»i thuyáº¿t trÃ¬nh
- Vai trÃ²: IT Researcher - Focus on ML Models

### Slide 2: Context
- Parkinson's Disease (PD) vÃ  váº¥n Ä‘á» Mild Cognitive Impairment (MCI)
- Táº§m quan trá»ng cá»§a dá»± Ä‘oÃ¡n sá»›m
- **Má»¥c tiÃªu**: So sÃ¡nh nhiá»u ML models Ä‘á»ƒ tÃ¬m phÆ°Æ¡ng phÃ¡p tá»‘t nháº¥t

---

## **PHáº¦N 2: PAPER PARKINSON - METHODS** (5-7 phÃºt)

### Slide 3: Dataset & Features
- **PPMI Study**: 423 patients, 4 years follow-up
- **Features** (40 predictors):
  - Clinical: Age, motor symptoms (UPDRS), cognitive tests (MoCA)
  - Biomarkers: CSF (AÎ²42, tau), neuroimaging (hippocampus volume)
- **Target**: PD-MCI vs PD-normal

### Slide 4: Models Compared in Paper
**6 Models tested:**
| Model | Type | Complexity |
|-------|------|-----------|
| Logistic Regression | Linear | Low |
| Support Vector Machine | Kernel-based | Medium |
| Random Forest | Ensemble | Medium |
| Gradient Boosting | Ensemble | Medium-High |
| Neural Network | Deep Learning | High |
| Ensemble Methods | Combined | High |

### Slide 5: Evaluation Metrics
- **AUC-ROC**: Primary metric
- **Sensitivity/Specificity**: Clinical utility
- **Calibration**: Prediction reliability
- **Feature Importance**: Interpretability

---

## **PHáº¦N 3: THá»°C NGHIá»†M MINH Há»ŒA** (8-10 phÃºt) â­ **PHáº¦N CHÃNH**

### Slide 6: Experimental Design - Progressive Model Evolution
> "Äá»ƒ hiá»ƒu sÃ¢u hÆ¡n vá» model selection, tÃ´i Ä‘Ã£ thá»±c hiá»‡n thá»±c nghiá»‡m tÆ°Æ¡ng tá»± trÃªn **Cardiovascular Disease prediction**"

**Dataset**: 70,000 patients, similar medical prediction task

**Progressive Framework (4 Generations):**
- **Gen 1 (Baseline)**: Logistic Regression, Decision Tree, KNN
- **Gen 2 (Ensemble)**: Random Forest, Extra Trees, GradientBoosting, SVM, MLP
- **Gen 3 (Advanced Boosting)**: XGBoost, LightGBM, CatBoost
- **Gen 4 (Deep Learning)**: PyTorch MLP, TabNet

**Total**: 11 models Ã— ~25 preprocessing configs = **270 experiments**

### Slide 7: Preprocessing Strategies Tested
**3 Preprocessing Dimensions:**

1. **Scaling** (3 methods):
   - Standard Scaler
   - Robust Scaler
   - None

2. **Imbalance Handling** (3 methods):
   - None
   - SMOTE (over-sampling)
   - SMOTE-ENN (hybrid)

3. **Feature Selection** (5 methods):
   - None
   - SelectKBest (k=5, k=12)
   - Mutual Information (k=5, k=12)

### Slide 8: Results - Generation Comparison
**BIá»‚U Äá»’ 1**: `1_generation_comparison.png`

| Generation | Mean PR-AUC | Best Model | Best AUC |
|------------|-------------|------------|----------|
| Gen 1 | 0.7576 | DecisionTree | 0.8023 |
| Gen 2 | 0.7653 | GradientBoosting | 0.7865 |
| Gen 3 | 0.7711 | CatBoost | 0.7864 |
| Gen 4 | 0.7660 | PyTorch MLP | 0.7839 |

**Key Findings:**
- âœ… Gen 3 (Advanced Boosting) cÃ³ mean performance cao nháº¥t
- âš ï¸ Gen 4 (Deep Learning) **KHÃ”NG** tá»‘t hÆ¡n (dataset size limitation)
- ğŸš€ Simple models (Gen 1) vá»›i **proper preprocessing** cÃ³ thá»ƒ vÆ°á»£t máº·t complex models!

### Slide 9: Preprocessing Impact
**BIá»‚U Äá»’ 2**: `2_preprocessing_impact.png`

**Best Configurations:**
- **Scaling**: NONE (PR-AUC: 0.7691)
  â†’ Tree-based models khÃ´ng cáº§n scaling!

- **Imbalance**: SMOTE-ENN (PR-AUC: 0.7678)
  â†’ Hybrid approach tá»‘t nháº¥t cho medical data

- **Feature Selection**: Mutual Info (k=12) (PR-AUC: 0.7723)
  â†’ Trade-off giá»¯a performance vÃ  interpretability

### Slide 10: Top 10 Configurations
**BIá»‚U Äá»’ 3**: `3_top10_models.png`

**Best Overall:**
- Model: **Decision Tree** (Gen 1)
- Config: **No scaling + SMOTE-ENN + Mutual Info (k=12)**
- **PR-AUC**: 0.8023 Â± 0.0041
- **Sensitivity**: 0.6900 (important for medical screening)
- **Specificity**: 0.7662
- **Training Time**: 22s (very fast!)

**Top 3:**
1. DecisionTree + SMOTE-ENN + MutualInfo-12: **0.8023**
2. KNN + SMOTE-ENN + SelectKBest-12: **0.8022**
3. KNN + SMOTE-ENN + MutualInfo-12: **0.8011**

**Insight**: All top 10 sá»­ dá»¥ng **SMOTE-ENN** for imbalance handling!

### Slide 11: Performance vs Training Time
**BIá»‚U Äá»’ 4**: `4_performance_vs_time.png`

**Trade-offs:**
- **Fast + Good**: Decision Tree (22s, AUC=0.8023)
- **Slow + Good**: CatBoost (513s, AUC=0.7864)
- **Very Slow + Good**: PyTorch MLP (310s, AUC=0.7839)

**Recommendation**: Choose based on deployment constraints
- Clinical practice: Fast models (Gen 1-2)
- Research: Advanced models (Gen 3-4) for best accuracy

---

## **PHáº¦N 4: PAPER RESULTS** (3-5 phÃºt)

### Slide 12: Paper Parkinson - Results
**Best Models from Paper:**

| Model | AUC | Sensitivity | Specificity |
|-------|-----|-------------|-------------|
| Random Forest | 0.78-0.82 | 0.75-0.80 | 0.70-0.75 |
| Gradient Boosting | 0.76-0.80 | 0.72-0.78 | 0.68-0.73 |
| Ensemble | 0.80-0.83 | 0.78-0.82 | 0.73-0.78 |

**Most Important Features:**
1. Baseline cognitive scores (MoCA, fluency)
2. CSF biomarkers (AÎ²42, tau)
3. Motor severity (UPDRS)
4. Age, education

### Slide 13: Comparison - Paper vs My Experiment
| Aspect | Paper (Parkinson) | My Experiment (CVD) |
|--------|-------------------|---------------------|
| Dataset Size | 423 patients | 70,000 patients |
| Features | 40 (clinical + biomarkers) | 15 (clinical) |
| Models | 6 models | 11 models (4 generations) |
| Best AUC | 0.80-0.83 | 0.7790 (ROC), 0.8023 (PR) |
| Best Model | Ensemble | Decision Tree + SMOTE-ENN |
| Key Finding | Biomarkers boost accuracy | Preprocessing > Model complexity |

**SO SÃNH:**
- âœ… My experiment AUC: 0.7790-0.8023 â†’ **TÆ¯Æ NG ÄÆ¯Æ NG** vá»›i paper!
- ğŸ’¡ Paper chÆ°a test advanced boosting (XGBoost, LightGBM, CatBoost)
- ğŸ”¬ Paper chÆ°a systematic preprocessing comparison

---

## **PHáº¦N 5: Äá»ŠNH HÆ¯á»šNG - CONTINUAL LEARNING** (5-7 phÃºt) â­ **TRá»ŒNG TÃ‚M**

### Slide 14: Current Limitations
**Váº¥n Ä‘á» vá»›i Current Approaches:**
1. **Single-site data**: Model chá»‰ há»c tá»« 1 bá»‡nh viá»‡n
2. **Static models**: KhÃ´ng update khi cÃ³ data má»›i
3. **Data privacy**: KhÃ´ng share patient data giá»¯a cÃ¡c sites
4. **Distribution shift**: Data khÃ¡c nhau giá»¯a cÃ¡c trung tÃ¢m

### Slide 15: Continual Learning - Giáº£i phÃ¡p

**Äá»‹nh nghÄ©a:**
> Continual Learning = Kháº£ nÄƒng há»c liÃªn tá»¥c tá»« data má»›i **KHÃ”NG QUÃŠN** knowledge cÅ©

**3 Challenges:**
1. **Catastrophic Forgetting**: Model quÃªn pattern cÅ© khi há»c data má»›i
2. **Data Heterogeneity**: KhÃ¡c biá»‡t distribution giá»¯a sites
3. **Class Imbalance Evolution**: Tá»· lá»‡ MCI thay Ä‘á»•i theo thá»i gian

### Slide 16: Continual Learning Approaches

**3 Strategies:**

**1. Regularization-based:**
- **Elastic Weight Consolidation (EWC)**:
  - Protect important weights khi update
  - Pháº¡t nhá»¯ng thay Ä‘á»•i lá»›n á»Ÿ parameters quan trá»ng

**2. Rehearsal-based:**
- **Experience Replay**:
  - LÆ°u subset cá»§a old data
  - Mix vá»›i new data khi train
- **Privacy-preserving**: Use synthetic data (GAN)

**3. Architecture-based:**
- **Progressive Neural Networks**:
  - ThÃªm columns má»›i cho má»—i task
- **Dynamic Networks**:
  - Expand capacity khi cáº§n

### Slide 17: Federated Learning + Continual Learning

**Integration Framework:**

```
Hospital A (Site 1) â”€â”€â”€â”€â”
  â”œâ”€ Local model        â”‚
  â”œâ”€ Local data         â”‚
  â””â”€ Continual update   â”‚
                        â”œâ”€â”€â†’ Aggregation â”€â”€â†’ Global Model
Hospital B (Site 2) â”€â”€â”€â”€â”¤                      (Continual)
  â”œâ”€ Local model        â”‚
  â”œâ”€ Local data         â”‚
  â””â”€ Continual update   â”‚
                        â”‚
Hospital C (Site 3) â”€â”€â”€â”€â”˜
  â”œâ”€ Local model
  â”œâ”€ Local data
  â””â”€ Continual update
```

**Process:**
1. Each site trains **local continual model**
2. Share only **model updates** (not data!) â†’ Privacy
3. Central server aggregates â†’ **Global continual model**
4. Distribute back to sites
5. Sites continue learning **WITHOUT forgetting** global knowledge

**Key Benefits:**
- âœ… Privacy-preserving (data stays local)
- âœ… Adapt to new patterns continuously
- âœ… Retain knowledge from all sites
- âœ… Handle distribution shift across hospitals

### Slide 18: Application to Parkinson MCI Prediction

**Scenario:**
- **Site A**: Data tá»« 2010-2015 (old biomarker protocols)
- **Site B**: Data tá»« 2016-2020 (updated protocols)
- **Site C**: Data tá»« 2021-2025 (new imaging techniques)

**Without Continual Learning:**
- Retrain from scratch â†’ Lose old knowledge
- Or keep static â†’ Miss new patterns

**With Continual Learning:**
- Model learns from Site B **WITHOUT** forgetting Site A
- Model adapts to Site C **WHILE** retaining A+B knowledge
- **Result**: Best of all worlds!

### Slide 19: Research Questions for Future Work

**Open Questions:**
1. Which CL algorithm works best for medical federated settings?
   - EWC vs Replay vs Progressive Networks?

2. How to balance **stability** (retain old) vs **plasticity** (learn new)?
   - Optimal hyperparameters for medical data?

3. Privacy-preserving rehearsal methods?
   - Synthetic data generation without patient info?

4. Benchmark datasets?
   - Need multi-center Parkinson datasets with temporal evolution

**Proposed Experiments:**
- Test CL algorithms on PPMI multi-site data
- Compare FL+CL vs traditional centralized training
- Measure catastrophic forgetting in medical models

---

## **PHáº¦N 6: Káº¾T LUáº¬N** (2-3 phÃºt)

### Slide 20: Key Takeaways

**1. Model Selection:**
- Advanced models (Boosting, Deep Learning) often best
- BUT: Simple models + good preprocessing can compete!
- Paper Parkinson: Ensemble (AUC 0.80-0.83)
- My Experiment: Decision Tree + SMOTE-ENN (PR-AUC 0.8023)

**2. Preprocessing Matters:**
- **Imbalance handling**: Critical for medical data
- **Feature selection**: Balance performance vs interpretability
- **Scaling**: Depends on model type (not needed for trees)

**3. Future Direction - Continual Learning:**
- Essential for **multi-center medical AI**
- Federated Learning + Continual Learning = **Scalable + Private**
- Research opportunities in:
  - Anti-forgetting techniques
  - Privacy-preserving methods
  - Multi-site validation

### Slide 21: References & Contact
- Paper: "Predicting Cognitive Decline in Parkinson's Disease" (Frontiers in Neuroscience, 2023)
- My Experiment Code: [GitHub Link]
- Contact: [Email]

**Thank you! Questions?**

---

## **PHá»¤ Lá»¤C: BACKUP SLIDES**

### Backup 1: Detailed Preprocessing Pipeline
- Step-by-step preprocessing trong experiment
- Code snippets

### Backup 2: Statistical Tests
- Wilcoxon signed-rank test for model comparison
- Cross-validation strategy

### Backup 3: Feature Importance Analysis
- Top features from best models
- SHAP values visualization

### Backup 4: Error Analysis
- Confusion matrices
- Failure cases analysis

---

## **TIMING BREAKDOWN (TOTAL: 25-30 phÃºt)**

| Pháº§n | Thá»i gian | Slides |
|------|-----------|--------|
| 1. Giá»›i thiá»‡u | 2-3 min | 1-2 |
| 2. Paper Methods | 5-7 min | 3-5 |
| 3. **Thá»±c nghiá»‡m** | **8-10 min** | **6-11** |
| 4. Paper Results | 3-5 min | 12-13 |
| 5. **Continual Learning** | **5-7 min** | **14-19** |
| 6. Káº¿t luáº­n | 2-3 min | 20-21 |
| Q&A | 5 min | - |

**Total**: 25-30 phÃºt + Q&A

---

## **NOTES CHO THUYáº¾T TRÃŒNH**

### Äiá»ƒm nháº¥n:
1. **Slide 8-10**: Thá»±c nghiá»‡m cá»§a báº¡n â†’ Show expertise
2. **Slide 16-18**: Continual Learning â†’ Innovation/Future work
3. Sá»­ dá»¥ng 4 biá»ƒu Ä‘á»“ tá»« `experiments/presentation_plots/`

### Tips:
- LÆ°á»›t nhanh pháº§n paper background (Slide 3-5)
- DÃ nh thá»i gian cho Slide 8-11 (thá»±c nghiá»‡m)
- **Emphasize** Slide 16-19 (Continual Learning - Ä‘á»‹nh hÆ°á»›ng chÃ­nh)
- Prepare backup slides cho Q&A technical

### CÃ¢u há»i dá»± kiáº¿n:
1. "Why not use deep learning?" â†’ Slide 8 (dataset size limitation)
2. "How to prevent catastrophic forgetting?" â†’ Slide 16 (EWC, Replay)
3. "Privacy concerns?" â†’ Slide 17 (Federated Learning)
4. "Practical deployment?" â†’ Slide 11 (Performance vs Time trade-off)
